{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Katona Máté (PD6YOR)\n",
    "\n",
    "# Homework 3\n",
    "\n",
    "The maximum score of this homework is 100+20 points. Grading is listed in this table:\n",
    "\n",
    "| Grade | Score range |\n",
    "| --- | --- |\n",
    "| 5 | 85+ |\n",
    "| 4 | 70-84 |\n",
    "| 3 | 55-69 |\n",
    "| 2 | 40-54 |\n",
    "| 1 | 0-39 |\n",
    "\n",
    "Most exercises include tests which should pass if your solution is correct.\n",
    "However, successful tests do not guarantee that your solution is correct.\n",
    "You are free to add more tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline\n",
    "\n",
    "Monday, 11 December 2017, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "\n",
    "Feel free to copy any boilerplate code you need from labs [9](../../course_material/09_Morphology_lab/09_Morphology_lab.ipynb#Morphology) and [10](../../course_material/10_Syntax/10_Syntax_lab.ipynb#Boilerplate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from functools import partial\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "def execute_commands(*cmds, fancy=True):\n",
    "    \"\"\"\n",
    "    Starts foma end executes the specified commands.\n",
    "    Might not work if there are too many...\n",
    "    \"\"\"\n",
    "    if fancy:\n",
    "        print('Executing commands...\\n=====================\\n')\n",
    "    args = ' '.join('-e \"{}\"'.format(cmd) for cmd in cmds)\n",
    "    output = subprocess.check_output('foma {} -s'.format(args),\n",
    "                                     stderr=subprocess.STDOUT,\n",
    "                                     shell=True).decode('utf-8')\n",
    "    print(output)\n",
    "    if fancy:\n",
    "        print('=====================\\n')\n",
    "    \n",
    "def compile_lexc(lexc_string, fst_file):\n",
    "    \"\"\"\n",
    "    Compiles a string describing a lexc lexicon with foma. The FST\n",
    "    is written to fst_file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='wt', encoding='utf-8', delete=False) as outf:\n",
    "            outf.write(lexc_string)\n",
    "    try:\n",
    "        execute_commands('read lexc {}'.format(outf.name),\n",
    "                         'save stack {}'.format(fst_file), fancy=False)\n",
    "        #!foma -e \"read lexc {outf.name}\" -e \"save stack {fst_file}\" -s\n",
    "    finally:\n",
    "        os.remove(outf.name)\n",
    "        \n",
    "def apply(fst_file, words, up=True):\n",
    "    \"\"\"\n",
    "    Applies the FST in fst_file on the supplied words. The default direction\n",
    "    is up.\n",
    "    \"\"\"\n",
    "    if isinstance(words, list):\n",
    "        words = '\\n'.join(map(str, words))\n",
    "    elif not isinstance(words, str):\n",
    "        raise ValueError('words must be a str or list')\n",
    "    header = 'Applying {} {}...'.format(fst_file, 'up' if up else 'down')\n",
    "    print('{}\\n{}\\n'.format(header, '=' * len(header)))\n",
    "    invert = '-i' if not up else ''\n",
    "    result = subprocess.check_output('flookup {} {}'.format(invert, fst_file),\n",
    "                                     stderr=subprocess.STDOUT, shell=True,\n",
    "                                     input=words.encode('utf-8'))\n",
    "    print(result.decode('utf-8')[:-1])  # Skip last newline\n",
    "    print('=' * len(header), '\\n')\n",
    "       \n",
    "apply_up = partial(apply, up=True)\n",
    "apply_down = partial(apply, up=False)\n",
    "\n",
    "def draw_net(fst_file, inline=True):\n",
    "    \"\"\"\n",
    "    Displays a compiled network inline or in a separate window.\n",
    "    The package imagemagic must be installed for this function to work.\n",
    "    \"\"\"\n",
    "    !foma -e \"load stack {fst_file}\" -e \"print dot >{fst_file}.dot\" -s\n",
    "    if inline:\n",
    "        png_data = subprocess.check_output(\n",
    "            'cat {}.dot | dot -Tpng'.format(fst_file), shell=True)\n",
    "        display(Image(data=png_data, format='png'))\n",
    "    else:\n",
    "        !cat {fst_file}.dot | dot -Tpng | display\n",
    "    !rm {fst_file}.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import nltk\n",
    "from nltk import Nonterminal\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def does_tcl_work():\n",
    "    \"\"\"Checks if Tcl is installed and works (e.g. it won't on a headless server).\"\"\"\n",
    "    tree = nltk.tree.Tree('test', [])\n",
    "    try:\n",
    "        tree._repr_png_()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def draw_tree(tree):\n",
    "    \"\"\"Draws an NLTK parse tree via Graphviz.\"\"\"\n",
    "    def draw_tree_rec(curr_root, graph, last_node):\n",
    "        node_id = str(int(last_node) + 1)\n",
    "        for child in curr_root:\n",
    "            if isinstance(child, nltk.tree.Tree):\n",
    "                graph.node(node_id, child.label(), penwidth='0')\n",
    "                graph.edge(last_node, node_id, color='darkslategray3', style='bold')\n",
    "                node_id = draw_tree_rec(child, graph, node_id)\n",
    "            else:\n",
    "                graph.node(node_id, child, penwidth='0')\n",
    "                graph.edge(last_node, node_id, color='darkslategray3', style='bold')\n",
    "                node_id = str(int(node_id) + 1)\n",
    "        return str(int(node_id) + 1)\n",
    "    \n",
    "    graph = graphviz.Graph()\n",
    "    graph.graph_attr['ranksep'] = '0.2'\n",
    "    graph.node('0', tree.label(), penwidth='0')\n",
    "    draw_tree_rec(tree, graph, '0')\n",
    "    return graph._repr_svg_()\n",
    "\n",
    "# Use Graphviz to draw the tree if the Tcl backend of nltk doesn't work\n",
    "if not does_tcl_work():\n",
    "    svg_formatter = get_ipython().display_formatter.formatters['image/svg+xml']\n",
    "    svg_formatter.for_type(nltk.tree.Tree, draw_tree)\n",
    "    # Delete the nltk drawing function, just to be sure\n",
    "    delattr(Tree, '_repr_png_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Morphology (45 points)\n",
    "\n",
    "\n",
    "## 1.1 Extend the lexc grammar (20 points)\n",
    "\n",
    "This exercise assumes that you have finished the lexc lab exercises. The grammar you hand in have to handle the adjectives listed in the lab exercises, as well as the number (singular / plural), the nominative and accusative cases, comparative and superlative forms, and vowel harmony.\n",
    "\n",
    "There are two sub-tasks to this exercise. You can download them from\n",
    "1. http://sandbox.mokk.bme.hu/~ndavid/homework3/{NEPTUN_CODE}/homework3_1_1_1.ipynb\n",
    "1. http://sandbox.mokk.bme.hu/~ndavid/homework3/{NEPTUN_CODE}/homework3_1_1_2.ipynb\n",
    "\n",
    "(Note: all letters in the Neptun code must be capitalized.)\n",
    "\n",
    "Please don't solve the exercises in the downloaded notebooks, but copy the descriptions and starter code snippets to the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Nouns\n",
    "\n",
    "Extend the grammar with nouns ending with a vowel.\n",
    "\n",
    "For the first group, the declension requires modification of the root: _**a**_ becomes _**á**_ and _**e**_ becomes _**é**_. For both groups, the linking vowel is the same as for adjectives.\n",
    "\n",
    "Examples:\n",
    "- _kutya_ + `[Pl]` $\\rightarrow$ _kuty**á**k_\n",
    "- _kutya_ + `[Acc]` $\\rightarrow$ _kuty**á**t_\n",
    "- _kutya_ + `[Pl]` + `[Acc]` $\\rightarrow$ _kuty**á**k**a**t_\n",
    "\n",
    "Hint: handle the inflected forms first and then find a shortcut for the `[Nom]` case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nouns_1 = \"\"\"\n",
    "kutya ! dog\n",
    "szalma ! straw\n",
    "utca ! street\n",
    "nénike ! little old lady\n",
    "öntöde ! foundry\n",
    "pecsenye ! roast\n",
    "\"\"\"\n",
    "\n",
    "nouns_2 = \"\"\"\n",
    "néni ! old lady\n",
    "tű ! needle\n",
    "randevú ! date (back)\n",
    "szajré ! swag (inf, back)\n",
    "jeti ! yeti\n",
    "faodú ! tree hollow (back)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root...2, nouns...4, fnouns...3, bnouns...3, fnouns_mod...3, bnouns_mod...3, mod...4, noun_tag...2, ex...2, sup...2, sup_tag...1, adjs...2, fadjs...9, badjs...8, adj_tag...1, comp...3, comp_tag...1, plur...4, plur_tag...1, case...4, case_tag...Building lexicon...\n",
      "1\n",
      "Determinizing...\n",
      "Minimizing...\n",
      "Done!\n",
      "5.0 kB. 159 states, 205 arcs, Cyclic.\n",
      "Writing to file nouns.fst.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"\n",
    "Multichar_Symbols \n",
    "    ! legesleg[/Supl]piros[/Adj|col]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
    "    ! nénike[/N]k[Pl]et[Acc]\n",
    "    [/Supl] [/Adj] [_Cmp/Adj] [Pl] [Acc] [/N]\n",
    "    @U.HARM.F@ @U.HARM.B@  ! harmony\n",
    "    @P.EX.T@ @R.EX@ @D.EX@  ! exaggarative \n",
    "    @P.SUP.T@ @R.SUP@ @D.SUP@  ! superlative\n",
    "    @P.VOW.T@ @R.VOW@ @D.VOW@ @C.VOW@  ! vowel ending\n",
    "    @P.MOD.T@ @P.MOD.F@ @R.MOD.T@ @R.MOD.F@ @D.MOD.T@ @D.MOD.F@ @C.MOD@  ! root modification\n",
    "\n",
    "LEXICON Root\n",
    "    @P.VOW.T@     nouns ;  ! in this example, all nouns end with a vowel\n",
    "                  ex    ;  ! will continue to adjs\n",
    "\n",
    "LEXICON nouns\n",
    "    @U.HARM.B@    bnouns     ;\n",
    "    @U.HARM.B@    bnouns_mod ;\n",
    "    @U.HARM.F@    fnouns     ;\n",
    "    @U.HARM.F@    fnouns_mod ;\n",
    "\n",
    "LEXICON fnouns\n",
    "    néni          noun_tag ;\n",
    "    tű            noun_tag ;\n",
    "    jeti          noun_tag ;\n",
    "\n",
    "LEXICON bnouns\n",
    "    randevú       noun_tag ;\n",
    "    szajré        noun_tag ;\n",
    "    faodú         noun_tag ;\n",
    "\n",
    "LEXICON fnouns_mod\n",
    "    nénik         mod ;\n",
    "    öntöd         mod ;\n",
    "    pecseny       mod ;\n",
    "\n",
    "LEXICON bnouns_mod\n",
    "    kuty          mod ;\n",
    "    szalm         mod ;\n",
    "    utc           mod ;\n",
    "\n",
    "LEXICON mod\n",
    "    @U.HARM.B@@P.MOD.F@a:@U.HARM.B@@P.MOD.F@a noun_tag ;\n",
    "    @U.HARM.B@@P.MOD.T@a:@U.HARM.B@@P.MOD.T@á noun_tag ;\n",
    "    @U.HARM.F@@P.MOD.F@e:@U.HARM.F@@P.MOD.F@e noun_tag ;\n",
    "    @U.HARM.F@@P.MOD.T@e:@U.HARM.F@@P.MOD.T@é noun_tag ;\n",
    "\n",
    "LEXICON noun_tag \n",
    "    @R.MOD.F@[/N]:@R.MOD.F@0 # ;  ! only non-modified stems (MOD.F) can stop\n",
    "    @D.MOD.F@[/N]:@D.MOD.F@0 plur ;\n",
    "\n",
    "LEXICON ex\n",
    "    @P.EX.T@leges ex  ;\n",
    "                  sup ;\n",
    "\n",
    "LEXICON sup\n",
    "    @P.SUP.T@leg  sup_tag ;\n",
    "    @D.EX@        adjs ;\n",
    "\n",
    "LEXICON sup_tag\n",
    "    [/Supl]:0    adjs ;\n",
    "\n",
    "LEXICON adjs\n",
    "    @U.HARM.F@    fadjs ;\n",
    "    @U.HARM.B@    badjs ;\n",
    "\n",
    "LEXICON fadjs\n",
    "    csendes       adj_tag ;\n",
    "    egészséges    adj_tag ;\n",
    "    idős          adj_tag ;\n",
    "    kék           adj_tag ;\n",
    "    mély          adj_tag ;\n",
    "    öntelt        adj_tag ;\n",
    "    szeles        adj_tag ;\n",
    "    terhes        adj_tag ;\n",
    "    zsémbes       adj_tag ;\n",
    "\n",
    "LEXICON badjs\n",
    "    abszurd       adj_tag ;\n",
    "    bájos         adj_tag ;\n",
    "    finom         adj_tag ;\n",
    "    gyanús        adj_tag ;\n",
    "    okos          adj_tag ;\n",
    "    piros         adj_tag ;\n",
    "    száraz        adj_tag ;\n",
    "    zord          adj_tag ;\n",
    "\n",
    "LEXICON adj_tag\n",
    "    [/Adj]:0       comp ;\n",
    "\n",
    "LEXICON comp \n",
    "    @D.SUP@       plur;\n",
    "    @U.HARM.F@ebb comp_tag ;\n",
    "    @U.HARM.B@abb comp_tag ;\n",
    "\n",
    "LEXICON comp_tag\n",
    "    [_Comp/Adj]:0  plur ;\n",
    "\n",
    "LEXICON plur\n",
    "                                       case ;\n",
    "    @R.VOW@@C.VOW@@C.MOD@k             plur_tag ;  ! only with vowel-ending words, but overrides both vowel-ending (C.VOW) and modified stem (C.MOD)\n",
    "    @D.VOW@@U.HARM.F@ek                plur_tag ;\n",
    "    @D.VOW@@U.HARM.B@ak                plur_tag ;\n",
    "\n",
    "LEXICON plur_tag\n",
    "    [Pl]:0        case ;\n",
    "\n",
    "LEXICON case\n",
    "    @D.MOD.T@            # ;  ! modified stems are not allowed to stop\n",
    "    @R.VOW@@C.VOW@t      case_tag ;\n",
    "    @D.VOW@@U.HARM.F@et  case_tag ;\n",
    "    @D.VOW@@U.HARM.B@at  case_tag ;\n",
    "\n",
    "LEXICON case_tag\n",
    "    [Acc]:0       # ;\n",
    "\"\"\"\n",
    "\n",
    "compile_lexc(grammar, 'nouns.fst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying nouns.fst up...\n",
      "========================\n",
      "\n",
      "nénikét\tnénike[/N]t[Acc]\n",
      "\n",
      "pecsenyék\tpecsenye[/N]k[Pl]\n",
      "\n",
      "tűket\ttű[/N]k[Pl]et[Acc]\n",
      "\n",
      "faodú\tfaodú[/N]\n",
      "\n",
      "legeslegeslegszárazabbakat\tlegeslegesleg[/Supl]száraz[/Adj]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
      "\n",
      "legterhesebb\tleg[/Supl]terhes[/Adj]ebb[_Comp/Adj]\n",
      "\n",
      "======================== \n",
      "\n",
      "Executing commands...\n",
      "=====================\n",
      "\n",
      "5.0 kB. 159 states, 205 arcs, Cyclic.\n",
      "kutya[/N]t[Acc]\n",
      "kutya[/N]k[Pl]at[Acc]\n",
      "kutya[/N]k[Pl]\n",
      "kutya[/N]\n",
      "randevú[/N]t[Acc]\n",
      "randevú[/N]k[Pl]at[Acc]\n",
      "randevú[/N]k[Pl]\n",
      "randevú[/N]\n",
      "utca[/N]t[Acc]\n",
      "utca[/N]k[Pl]at[Acc]\n",
      "utca[/N]k[Pl]\n",
      "utca[/N]\n",
      "szajré[/N]t[Acc]\n",
      "szajré[/N]k[Pl]at[Acc]\n",
      "szajré[/N]k[Pl]\n",
      "szajré[/N]\n",
      "szalma[/N]t[Acc]\n",
      "szalma[/N]k[Pl]at[Acc]\n",
      "szalma[/N]k[Pl]\n",
      "szalma[/N]\n",
      "faodú[/N]t[Acc]\n",
      "faodú[/N]k[Pl]at[Acc]\n",
      "faodú[/N]k[Pl]\n",
      "faodú[/N]\n",
      "tű[/N]t[Acc]\n",
      "tű[/N]k[Pl]et[Acc]\n",
      "tű[/N]k[Pl]\n",
      "tű[/N]\n",
      "nénike[/N]t[Acc]\n",
      "nénike[/N]k[Pl]et[Acc]\n",
      "nénike[/N]k[Pl]\n",
      "nénike[/N]\n",
      "néni[/N]t[Acc]\n",
      "néni[/N]k[Pl]et[Acc]\n",
      "néni[/N]k[Pl]\n",
      "néni[/N]\n",
      "jeti[/N]t[Acc]\n",
      "jeti[/N]k[Pl]et[Acc]\n",
      "jeti[/N]k[Pl]\n",
      "jeti[/N]\n",
      "pecsenye[/N]t[Acc]\n",
      "pecsenye[/N]k[Pl]et[Acc]\n",
      "pecsenye[/N]k[Pl]\n",
      "pecsenye[/N]\n",
      "öntöde[/N]t[Acc]\n",
      "öntöde[/N]k[Pl]et[Acc]\n",
      "öntöde[/N]k[Pl]\n",
      "öntöde[/N]\n",
      "abszurd[/Adj]abb[_Comp/Adj]at[Acc]\n",
      "abszurd[/Adj]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
      "abszurd[/Adj]abb[_Comp/Adj]ak[Pl]\n",
      "abszurd[/Adj]abb[_Comp/Adj]\n",
      "abszurd[/Adj]at[Acc]\n",
      "abszurd[/Adj]ak[Pl]at[Acc]\n",
      "abszurd[/Adj]ak[Pl]\n",
      "abszurd[/Adj]\n",
      "bájos[/Adj]abb[_Comp/Adj]at[Acc]\n",
      "bájos[/Adj]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
      "bájos[/Adj]abb[_Comp/Adj]ak[Pl]\n",
      "bájos[/Adj]abb[_Comp/Adj]\n",
      "bájos[/Adj]at[Acc]\n",
      "bájos[/Adj]ak[Pl]at[Acc]\n",
      "bájos[/Adj]ak[Pl]\n",
      "bájos[/Adj]\n",
      "okos[/Adj]abb[_Comp/Adj]at[Acc]\n",
      "okos[/Adj]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
      "okos[/Adj]abb[_Comp/Adj]ak[Pl]\n",
      "okos[/Adj]abb[_Comp/Adj]\n",
      "okos[/Adj]at[Acc]\n",
      "okos[/Adj]ak[Pl]at[Acc]\n",
      "okos[/Adj]ak[Pl]\n",
      "okos[/Adj]\n",
      "zord[/Adj]abb[_Comp/Adj]at[Acc]\n",
      "zord[/Adj]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
      "zord[/Adj]abb[_Comp/Adj]ak[Pl]\n",
      "zord[/Adj]abb[_Comp/Adj]\n",
      "zord[/Adj]at[Acc]\n",
      "zord[/Adj]ak[Pl]at[Acc]\n",
      "zord[/Adj]ak[Pl]\n",
      "zord[/Adj]\n",
      "száraz[/Adj]abb[_Comp/Adj]at[Acc]\n",
      "száraz[/Adj]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
      "száraz[/Adj]abb[_Comp/Adj]ak[Pl]\n",
      "száraz[/Adj]abb[_Comp/Adj]\n",
      "száraz[/Adj]at[Acc]\n",
      "száraz[/Adj]ak[Pl]at[Acc]\n",
      "száraz[/Adj]ak[Pl]\n",
      "száraz[/Adj]\n",
      "gyanús[/Adj]abb[_Comp/Adj]at[Acc]\n",
      "gyanús[/Adj]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
      "gyanús[/Adj]abb[_Comp/Adj]ak[Pl]\n",
      "gyanús[/Adj]abb[_Comp/Adj]\n",
      "gyanús[/Adj]at[Acc]\n",
      "gyanús[/Adj]ak[Pl]at[Acc]\n",
      "gyanús[/Adj]ak[Pl]\n",
      "gyanús[/Adj]\n",
      "piros[/Adj]abb[_Comp/Adj]at[Acc]\n",
      "piros[/Adj]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
      "piros[/Adj]abb[_Comp/Adj]ak[Pl]\n",
      "piros[/Adj]abb[_Comp/Adj]\n",
      "\n",
      "=====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# draw_net('nouns.fst')\n",
    "apply_up('nouns.fst', ['nénikét', 'pecsenyék', 'tűket', 'faodú', 'legeslegeslegszárazabbakat', 'legterhesebb'])        \n",
    "execute_commands('load stack nouns.fst', 'print upper-words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 The allative case\n",
    "\n",
    "Extend the grammar with the allative case. This case differs from accusative in two ways:\n",
    "- it never gets a linking vowel\n",
    "- it has three forms (_-hoz_ / _-hez_ / _-höz_). The first two are used for the words you would expect. The third one is used for words which takes front-vowel inflections, but whose last vowel is a _**ö**_ or _**ő**_ (yes, there is only one of those in the lab word lists).\n",
    "\n",
    "Examples:\n",
    "- _finom_ + `[All]` $\\rightarrow$ _finom**hoz**_\n",
    "- _mély_ + `[All]` $\\rightarrow$ _mély**hez**_\n",
    "- _idős_ + `[All]` $\\rightarrow$ _idős**höz**_\n",
    "\n",
    "Also add the following adjectives to the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adjectives = \"\"\"\n",
    "bűnös ! sinful\n",
    "ösztönös ! instinctive\n",
    "felnőtt ! adult\n",
    "dühös ! angry\n",
    "erős ! strong\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root...1, ex...2, sup...2, sup_tag...1, adjs...3, fadjs...8, badjs...8, oadjs...6, adj_tag...1, comp...3, comp_tag...1, plur...3, plur_tag...1, case...6, acc_tag...1, all_tag...Building lexicon...\n",
      "1\n",
      "Determinizing...\n",
      "Minimizing...\n",
      "Done!\n",
      "4.0 kB. 122 states, 156 arcs, Cyclic.\n",
      "Writing to file allative.fst.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"\n",
    "Multichar_Symbols \n",
    "    ! erős[/Adj]höz[All]\t\n",
    "    [/Supl] [/Adj] [_Comp/Adj] [Pl] [Acc]\n",
    "    @P.HARM.F@ @P.HARM.B@ @P.HARM.O@ @D.HARM.B@ @R.HARM.F@ @R.HARM.B@ @R.HARM.O@  ! harmony\n",
    "    @P.EX.T@ @R.EX@ @D.EX@  ! exaggarative\n",
    "    @P.SUP.T@ @R.SUP@ @D.SUP@  ! superlative\n",
    "\n",
    "LEXICON Root\n",
    "                  ex    ;  ! will continue to adjs\n",
    "\n",
    "LEXICON ex\n",
    "    @P.EX.T@leges ex  ;\n",
    "                  sup ;\n",
    "\n",
    "LEXICON sup\n",
    "    @P.SUP.T@leg  sup_tag ;\n",
    "    @D.EX@        adjs ;\n",
    "\n",
    "LEXICON sup_tag\n",
    "    [/Supl]:0    adjs ;\n",
    "\n",
    "LEXICON adjs\n",
    "    @P.HARM.F@    fadjs ;\n",
    "    @P.HARM.B@    badjs ;\n",
    "    @P.HARM.O@    oadjs ;\n",
    "\n",
    "LEXICON fadjs\n",
    "    csendes       adj_tag ;\n",
    "    egészséges    adj_tag ;\n",
    "!    idős          adj_tag ;  ! moved to oeadjs\n",
    "    kék           adj_tag ;\n",
    "    mély          adj_tag ;\n",
    "    öntelt        adj_tag ;\n",
    "    szeles        adj_tag ;\n",
    "    terhes        adj_tag ;\n",
    "    zsémbes       adj_tag ;\n",
    "\n",
    "LEXICON badjs\n",
    "    abszurd       adj_tag ;\n",
    "    bájos         adj_tag ;\n",
    "    finom         adj_tag ;\n",
    "    gyanús        adj_tag ;\n",
    "    okos          adj_tag ;\n",
    "    piros         adj_tag ;\n",
    "    száraz        adj_tag ;\n",
    "    zord          adj_tag ;\n",
    "\n",
    "LEXICON oadjs\n",
    "    idős          adj_tag ;\n",
    "    bűnös         adj_tag ;\n",
    "    ösztönös      adj_tag ;\n",
    "    felnőtt       adj_tag ;\n",
    "    dühös         adj_tag ;\n",
    "    erős          adj_tag ;\n",
    "\n",
    "LEXICON adj_tag\n",
    "    [/Adj]:0       comp ;\n",
    "\n",
    "LEXICON comp \n",
    "    @D.SUP@                 plur;\n",
    "    @D.HARM.B@@P.HARM.F@ebb comp_tag ;\n",
    "    @R.HARM.B@abb           comp_tag ;\n",
    "\n",
    "LEXICON comp_tag\n",
    "    [_Comp/Adj]:0  plur ;\n",
    "\n",
    "LEXICON plur\n",
    "                                case ;\n",
    "    @D.HARM.B@@P.HARM.F@ek      plur_tag ;\n",
    "    @R.HARM.B@ak                plur_tag ;\n",
    "\n",
    "LEXICON plur_tag\n",
    "    [Pl]:0        case ;\n",
    "\n",
    "LEXICON case\n",
    "                   # ;\n",
    "    @R.HARM.F@hez  all_tag ;\n",
    "    @R.HARM.B@hoz  all_tag ;\n",
    "    @R.HARM.O@höz  all_tag ;\n",
    "    @D.HARM.B@et   acc_tag ;\n",
    "    @R.HARM.B@at   acc_tag ;\n",
    "\n",
    "LEXICON acc_tag\n",
    "    [Acc]:0       # ;\n",
    "\n",
    "LEXICON all_tag\n",
    "    [All]:0       # ;\n",
    "\"\"\"\n",
    "\n",
    "compile_lexc(grammar, 'allative.fst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying allative.fst up...\n",
      "===========================\n",
      "\n",
      "erősebbekhez\terős[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "\n",
      "erőshöz\terős[/Adj]höz[All]\n",
      "\n",
      "legeslegeslegpirosabbhoz\tlegeslegesleg[/Supl]piros[/Adj]abb[_Comp/Adj]hoz[All]\n",
      "\n",
      "kéket\tkék[/Adj]et[Acc]\n",
      "\n",
      "=========================== \n",
      "\n",
      "Executing commands...\n",
      "=====================\n",
      "\n",
      "4.0 kB. 122 states, 156 arcs, Cyclic.\n",
      "terhes[/Adj]ebb[_Comp/Adj]\n",
      "terhes[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "terhes[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "terhes[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "terhes[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "terhes[/Adj]ebb[_Comp/Adj]hez[All]\n",
      "terhes[/Adj]\n",
      "terhes[/Adj]et[Acc]\n",
      "terhes[/Adj]ek[Pl]\n",
      "terhes[/Adj]ek[Pl]et[Acc]\n",
      "terhes[/Adj]ek[Pl]hez[All]\n",
      "terhes[/Adj]hez[All]\n",
      "egészséges[/Adj]ebb[_Comp/Adj]\n",
      "egészséges[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "egészséges[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "egészséges[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "egészséges[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "egészséges[/Adj]ebb[_Comp/Adj]hez[All]\n",
      "egészséges[/Adj]\n",
      "egészséges[/Adj]et[Acc]\n",
      "egészséges[/Adj]ek[Pl]\n",
      "egészséges[/Adj]ek[Pl]et[Acc]\n",
      "egészséges[/Adj]ek[Pl]hez[All]\n",
      "egészséges[/Adj]hez[All]\n",
      "öntelt[/Adj]ebb[_Comp/Adj]\n",
      "öntelt[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "öntelt[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "öntelt[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "öntelt[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "öntelt[/Adj]ebb[_Comp/Adj]hez[All]\n",
      "öntelt[/Adj]\n",
      "öntelt[/Adj]et[Acc]\n",
      "öntelt[/Adj]ek[Pl]\n",
      "öntelt[/Adj]ek[Pl]et[Acc]\n",
      "öntelt[/Adj]ek[Pl]hez[All]\n",
      "öntelt[/Adj]hez[All]\n",
      "zsémbes[/Adj]ebb[_Comp/Adj]\n",
      "zsémbes[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "zsémbes[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "zsémbes[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "zsémbes[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "zsémbes[/Adj]ebb[_Comp/Adj]hez[All]\n",
      "zsémbes[/Adj]\n",
      "zsémbes[/Adj]et[Acc]\n",
      "zsémbes[/Adj]ek[Pl]\n",
      "zsémbes[/Adj]ek[Pl]et[Acc]\n",
      "zsémbes[/Adj]ek[Pl]hez[All]\n",
      "zsémbes[/Adj]hez[All]\n",
      "kék[/Adj]ebb[_Comp/Adj]\n",
      "kék[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "kék[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "kék[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "kék[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "kék[/Adj]ebb[_Comp/Adj]hez[All]\n",
      "kék[/Adj]\n",
      "kék[/Adj]et[Acc]\n",
      "kék[/Adj]ek[Pl]\n",
      "kék[/Adj]ek[Pl]et[Acc]\n",
      "kék[/Adj]ek[Pl]hez[All]\n",
      "kék[/Adj]hez[All]\n",
      "szeles[/Adj]ebb[_Comp/Adj]\n",
      "szeles[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "szeles[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "szeles[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "szeles[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "szeles[/Adj]ebb[_Comp/Adj]hez[All]\n",
      "szeles[/Adj]\n",
      "szeles[/Adj]et[Acc]\n",
      "szeles[/Adj]ek[Pl]\n",
      "szeles[/Adj]ek[Pl]et[Acc]\n",
      "szeles[/Adj]ek[Pl]hez[All]\n",
      "szeles[/Adj]hez[All]\n",
      "mély[/Adj]ebb[_Comp/Adj]\n",
      "mély[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "mély[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "mély[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "mély[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "mély[/Adj]ebb[_Comp/Adj]hez[All]\n",
      "mély[/Adj]\n",
      "mély[/Adj]et[Acc]\n",
      "mély[/Adj]ek[Pl]\n",
      "mély[/Adj]ek[Pl]et[Acc]\n",
      "mély[/Adj]ek[Pl]hez[All]\n",
      "mély[/Adj]hez[All]\n",
      "csendes[/Adj]ebb[_Comp/Adj]\n",
      "csendes[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "csendes[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "csendes[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "csendes[/Adj]ebb[_Comp/Adj]ek[Pl]hez[All]\n",
      "csendes[/Adj]ebb[_Comp/Adj]hez[All]\n",
      "csendes[/Adj]\n",
      "csendes[/Adj]et[Acc]\n",
      "csendes[/Adj]ek[Pl]\n",
      "csendes[/Adj]ek[Pl]et[Acc]\n",
      "csendes[/Adj]ek[Pl]hez[All]\n",
      "csendes[/Adj]hez[All]\n",
      "erős[/Adj]ebb[_Comp/Adj]\n",
      "erős[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "erős[/Adj]ebb[_Comp/Adj]ek[Pl]\n",
      "erős[/Adj]ebb[_Comp/Adj]ek[Pl]et[Acc]\n",
      "\n",
      "=====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# draw_net('allative.fst')\n",
    "apply_up('allative.fst', ['erősebbekhez', 'erőshöz', 'legeslegeslegpirosabbhoz', 'kéket'])        \n",
    "execute_commands('load stack allative.fst', 'print upper-words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CFG morphology (25 points)\n",
    "\n",
    "### 1.2.1 The basics (15 points)\n",
    "\n",
    "Implement morphological analysis with a CFG grammar. Requirements:\n",
    "- the grammar should handle everything we covered in the lab, except for vowel harmony\n",
    "- there is no need to handle generation; concentrate on parsing\n",
    "- it doesn't matter what the intermediate nonterminals are called. Ideally, _preterminals_ should be valid morphological tags, as in the example below; however, `nlkt.CFG.fromstring()` cannot parse tags such as `[Acc]` as nonterminals. You are free to call them whatever you wish, but descriptive names are encouraged.\n",
    "- encapsulate the functionality in a `CFGMorphParser` class:\n",
    "    - its `__init__()` should accept no parameters (or at least provide defaults)\n",
    "    - it should have a `parse_tree()` method, which accepts a word and returns the parse tree\n",
    "    - it should have a `parse()` method, which accepts a word and returns the morphological parse _in the same form as HFST_. Refer to `hfst_lookup` and the tests for the format. `nltk.tree.Tree.pos()` is a good starting point\n",
    "\n",
    "Note that having the same format as HFST doesn't mean you have to return the exact same output: for instance, we defined _terhes_ as a genuine adjective, even though it is derived from the noun _teher_. So HFST would analyze it as `teher[/N]es[_Adjz:s/Adj][Nom]`, but you only need to return `terhes[/Adj]`. You **also don't have to cover [Nom]**, because of [this bug](https://github.com/nltk/nltk/issues/1890)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('__NOM_1', [Tree('__NOM_2', [Tree('__SUPL_ADJ_2', [Tree('[Supl]', ['leg']), Tree('[/Adj]', ['nagy']), Tree('[_Comp/Adj]', ['obb'])]), Tree('[Pl]', ['ak'])]), Tree('[Acc]', ['at'])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "tree = Tree('__NOM_1', [\n",
    "    Tree('__NOM_2', [\n",
    "        Tree('__SUPL_ADJ_2', [\n",
    "            Tree('[Supl]', ['leg']),\n",
    "            Tree('[/Adj]', ['nagy']),\n",
    "            Tree('[_Comp/Adj]', ['obb']),\n",
    "        ]),\n",
    "        Tree('[Pl]', ['ak']),\n",
    "    ]),\n",
    "    Tree('[Acc]', ['at']),\n",
    "])\n",
    "\n",
    "display(tree)\n",
    "# If display() doesn't work, try this\n",
    "# tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "terhes[/Adj]\n",
      "csendes[/Adj]ebb[_Comp/Adj]et[Acc]\n",
      "leg[/Supl]finom[/Adj]abb[_Comp/Adj]ak[Pl]\n",
      "legesleg[/Supl]finom[/Adj]abb[_Comp/Adj]ek[Pl]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('__Nom', [Tree('__Nom2', [Tree('__AdjComp', [Tree('__ExSupl', [Tree('__Ex', [Tree('__Ex', [Tree('__Ex', ['l', 'e', 'g', 'e', 's']), Tree('__Ex', ['l', 'e', 'g', 'e', 's'])]), Tree('__Ex', ['l', 'e', 'g', 'e', 's'])]), Tree('__Supl', ['l', 'e', 'g'])]), Tree('__Adj', ['f', 'i', 'n', 'o', 'm']), Tree('__Comp', ['a', 'b', 'b'])]), Tree('__Pl', ['e', 'k'])])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your implementation here\n",
    "\n",
    "\n",
    "# based on nltk.tree.Tree.pos\n",
    "def pos(tree):\n",
    "    p = []\n",
    "    for child in tree:\n",
    "        if isinstance(child, Tree):\n",
    "            p.extend(pos(child))\n",
    "        else:\n",
    "            return ''.join(tree.leaves()) + tree.label()\n",
    "    return p\n",
    "\n",
    "    \n",
    "class CFGMorphParser(object):\n",
    "    tag_map = {\n",
    "        '__Acc': '[Acc]',\n",
    "        '__Pl': '[Pl]',\n",
    "        '__Comp': '[_Comp/Adj]',\n",
    "        '__Adj': '[/Adj]',\n",
    "        '__Supl': '[/Supl]',\n",
    "        '__Ex': '',\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grammar = nltk.CFG.fromstring(\"\"\"\n",
    "        __Nom -> __Nom2 __Acc | __Nom2\n",
    "        __Nom2 -> __AdjComp __Pl | __AdjComp\n",
    "        __AdjComp -> __ExSupl __Adj __Comp | __Adj __Comp | __Adj\n",
    "        __ExSupl -> __Ex __Supl | __Supl\n",
    "        __Ex -> __Ex __Ex | 'l' 'e' 'g' 'e' 's'\n",
    "        __Supl -> 'l' 'e' 'g'\n",
    "        __Adj -> 't' 'e' 'r' 'h' 'e' 's' | 'c' 's' 'e' 'n' 'd' 'e' 's' | 'f' 'i' 'n' 'o' 'm'\n",
    "        __Comp -> 'a' 'b' 'b' | 'e' 'b' 'b'\n",
    "        __Pl -> 'a' 'k' | 'e' 'k'\n",
    "        __Acc -> 'a' 't' | 'e' 't'\n",
    "        \"\"\")\n",
    "        self.p = nltk.ChartParser(self.grammar)\n",
    "        \n",
    "    def parse_tree(self, word):\n",
    "        try:\n",
    "            trees = self.p.parse(word)\n",
    "        except ValueError as e:\n",
    "            return None\n",
    "        for tree in trees:\n",
    "            return tree\n",
    "    \n",
    "    def parse(self, word):\n",
    "        tree = self.parse_tree(word)\n",
    "        if tree is None: \n",
    "            return None\n",
    "        morph = ''.join(pos(tree))\n",
    "        # replace tags with hfst equivalent\n",
    "        for t, p in self.tag_map.items():\n",
    "            morph = morph.replace(t, p)\n",
    "        return morph\n",
    "        \n",
    "    \n",
    "# Tests\n",
    "parser = CFGMorphParser()\n",
    "\n",
    "print(parser.parse('unknown_word'))\n",
    "print(parser.parse('terhes'))\n",
    "print(parser.parse('csendesebbet'))\n",
    "print(parser.parse('legfinomabbak'))\n",
    "print(parser.parse('legeslegfinomabbek'))\n",
    "display(parser.parse_tree('legeslegeslegeslegfinomabbak'))\n",
    "\n",
    "assert parser.parse('unknown_word') is None\n",
    "assert parser.parse('terhes') == 'terhes[/Adj]'\n",
    "assert parser.parse('csendesebbet') == 'csendes[/Adj]ebb[_Comp/Adj]et[Acc]'\n",
    "assert parser.parse('legfinomabbak') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ak[Pl]'\n",
    "# It's OK here\n",
    "assert parser.parse('legfinomabbek') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ek[Pl]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Very nice. The only small problem is that \"_legesleges_\" is not supported.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Vowel harmony (10 points)\n",
    "\n",
    "Also handle vowel harmony. Write a function that traverses the tree manually (similarly to [exercise 2.4 in the lab](../../course_material/10_Syntax/10_Syntax_lab_solutions.ipynb#2.4-Evaluation*)) and returns `True` or `False`, depending on whether the tree conforms to vowel harmony rules. Use this function in `parse_tree` (and `parse`) to filter invalid trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert parser.parse('legfinomabbak') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ak[Pl]'\n",
    "assert parser.parse('legfinomabbek') == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Syntax (55 points)\n",
    "\n",
    "In this exercise, you will parse a treebank, and induce a PCFG grammar from it. You will then implement a probabilistic version of the CKY algorithm, and evaluate the grammar on the test split of the treebank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Parse a treebank (10 points)\n",
    "\n",
    "Parse the treebank file `en_lines-ud-train.s` in the notebook's directory. Write a **generator** function that reads the file and yields `nltk.tree.Tree` objects. In particular,\n",
    "- do not read the whole file into memory\n",
    "- the `Tree.fromstring()` function converts an s-expression into a tree\n",
    "\n",
    "Open the file in an editor to see the formatting.\n",
    "\n",
    "Note that the file was created by parsing the [LinES dependency corpus](https://github.com/UniversalDependencies/UD_English-LinES/tree/master) with [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/), so it is not a gold standard by any means, but it will suffice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "\n",
    "def parse_treebank(treebank_file):\n",
    "    with open(treebank_file, 'r') as f:\n",
    "        s = ''\n",
    "        for l in f:\n",
    "            if len(l) > 1:\n",
    "                s += l\n",
    "            elif s:\n",
    "                try:\n",
    "                    t = Tree.fromstring(s)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                finally:\n",
    "                    s = ''\n",
    "                yield t\n",
    "            else:\n",
    "                s = ''\n",
    "            \n",
    "# Tests\n",
    "assert sum(1 for _ in parse_treebank('en_lines-ud-train.s')) == 2613\n",
    "assert isinstance(next(parse_treebank('en_lines-ud-train.s')), Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">OK.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filter trees (5 points)\n",
    "\n",
    "In order to avoid problems further down the line, we shall only handle a subset of the trees in the treebank. We call a tree _valid_, if\n",
    "- its root is `'S'`\n",
    "- the root has at least two children.\n",
    "\n",
    "Write a function that returns `True` for \"valid\" trees and `False` for invalid ones. Filter the your generator with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_tree_valid(tree):\n",
    "    if not isinstance(tree, Tree):\n",
    "        return False\n",
    "    if tree.label() != 'S':\n",
    "        return False\n",
    "    if len(tree) < 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Tests\n",
    "assert sum(map(is_tree_valid, parse_treebank('en_lines-ud-train.s'))) == 2311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Induce the PCFG grammar (10 points)\n",
    "\n",
    "Now that you have the trees, it is time to induce (train) a PCFG grammar for it! Luckily, `nltk` has a functions for just that: [`nltk.grammar.induce_pcfg`](http://www.nltk.org/api/nltk.html#nltk.grammar.induce_pcfg). Use it to acquire your PCFG grammar. You can find hints at how to use it in the [grammar module](http://www.nltk.org/_modules/nltk/grammar.html).\n",
    "\n",
    "Note: since we want to parse sentences with the PCKY algorithm, we need our grammar to be in CNF. Unfortunately, `nlkt` cannot convert a grammar to CNF, so you have to ensure that the trees are in CNF before feeding them to the PCFG induction function. That way, we can be sure that our grammar will be also. There are two functions that ensure a tree is in CNF:\n",
    "- [`collapse_unary`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.collapse_unary). Make sure you call it with `collapsePOS=True`!\n",
    "- [`chomsky_normal_form`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.chomsky_normal_form). Do not use any smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_grammar(trees):\n",
    "    prods = []\n",
    "    for tree in trees: \n",
    "        tree.collapse_unary(collapsePOS=True)\n",
    "        tree.chomsky_normal_form()\n",
    "        prods += tree.productions()\n",
    "    return nltk.grammar.induce_pcfg(nltk.Nonterminal('S'), prods)\n",
    "        \n",
    "def is_grammar_cnf(grammar):\n",
    "    for prod in grammar.productions():\n",
    "        rhs = prod.rhs()\n",
    "        if len(rhs) > 2 or (len(rhs) == 1 and isinstance(rhs[0], nltk.Nonterminal)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Tests\n",
    "grammar = train_grammar(filter(is_tree_valid, parse_treebank('en_lines-ud-train.s')))\n",
    "assert is_grammar_cnf(grammar)\n",
    "assert len(grammar.productions()) == 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Implement PCKY (15 points)\n",
    "\n",
    "Implement the PCKY algorithm. Encapsulate it in a class called `PCKYParser`. Extend your `CKYParser` solution from the lab so that it creates trees with probabilities (`ProbabilisticTree`). The `parse()` method should also accept a parameter `n`, and only return the most probable `n` trees (as a generator).\n",
    "\n",
    "Some pointers:\n",
    "- [ProbabilisticTree](http://www.nltk.org/api/nltk.html#nltk.tree.ProbabilisticTree), which inherits from\n",
    "- [ProbabilisticMixIn](http://www.nltk.org/api/nltk.html#nltk.probability.ProbabilisticMixIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tree import ProbabilisticTree as PTree\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class PCKYParser(object):\n",
    "    def __init__(self, grammar):\n",
    "        if not isinstance(grammar, nltk.CFG):\n",
    "            raise TypeError(\"g\")\n",
    "        self.grammar = grammar\n",
    "        # save some time by separating lexical and production rules\n",
    "        self.lexicals = [prod for prod in self.grammar.productions() if len(prod.rhs()) == 1]\n",
    "        self.productions = [prod for prod in self.grammar.productions() if len(prod.rhs()) == 2]\n",
    "    \n",
    "    def parse(self, sent, n=1):\n",
    "        k = len(sent)\n",
    "        # init\n",
    "        cky = np.empty((k,k), dtype=object)\n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                cky[i, j] = []\n",
    "        # lexical rules\n",
    "        for i, word in enumerate(sent):\n",
    "            for lex in self.lexicals:\n",
    "                if word in lex.rhs():\n",
    "                    cky[i, i].append(PTree(lex.lhs(), [word], prob=lex.prob()))\n",
    "        # production rules\n",
    "        for col in range(1, k):\n",
    "            for row in range(col-1, -1, -1):\n",
    "                ways_to_split = col-row  # \"distance\" from diag\n",
    "                for w in range(ways_to_split):\n",
    "                    left = ways_to_split - w\n",
    "                    down = 1 + w\n",
    "                    for prod in self.productions:\n",
    "                        # because of CNF, all non-lexical rules have 2 constituents on rhs\n",
    "                        ls = prod.rhs()[0]  # left symbol\n",
    "                        ds = prod.rhs()[1]  # down symbol\n",
    "                        for lt in cky[row, col-left]:  # left tree\n",
    "                            for dt in cky[row+down, col]:  # right tree\n",
    "                                if lt.label() == ls and dt.label() == ds:\n",
    "                                    # multiply tree and production probabilities\n",
    "                                    p = lt.prob() * dt.prob() * prod.prob()\n",
    "                                    cky[row, col].append(PTree(prod.lhs(), [lt, dt], prob=p))\n",
    "        # collect sentences from parsed trees, sort them by probability\n",
    "        sents = sorted(\n",
    "            [tree for tree in cky[0, k-1] if tree.label() == nltk.Nonterminal('S')],\n",
    "            key=lambda tree: tree.prob()\n",
    "        )\n",
    "        # yield the most probable n trees\n",
    "        for sent in deque(sents, maxlen=n):\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = train_grammar(filter(is_tree_valid, parse_treebank('en_lines-ud-train.s')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCKYParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6e50c34b1e6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# tests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCKYParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'A user writes'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all trees:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrees\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCKYParser' is not defined"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "pparser = PCKYParser(grammar)\n",
    "trees = pparser.parse('A user writes'.split(), n=3)\n",
    "print('all trees:')\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    display(tree)\n",
    "print('-'*80)\n",
    "# let's see if it returns the most probable tree when n=1\n",
    "trees = pparser.parse('A user writes'.split(), n=1)\n",
    "print('most probable tree:')\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Great! Once sorted, though, you could have just returned `sents[:n]`, no point in used the `deque`.</span>\n",
    "\n",
    "<span style=\"color:green\">Also, you have realized yourself that it is slow. It is possible to call `grammar.productions()` with the `rhs=XXX` keyword, in which case it does the lookup for you, so that it isn't that slow. Alternatively, you could have created a dictionary that does it for you: just enumerating all 15000 rules won't cut it speedwise, that's for sure.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Evaluate the grammar (15 points)\n",
    "\n",
    "Evaluate your grammar on the test split of the treebank (`en_lines-ud-dev.s`). Implement the **unlabelled** PARSEVAL metric. See [the first answer for an example](https://linguistics.stackexchange.com/questions/1873/is-there-a-well-established-metric-to-measure-the-effectiveness-of-a-parsing-alg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constituents(tree):\n",
    "    consts = []\n",
    "    if isinstance(tree, Tree) and isinstance(tree[0], Tree):\n",
    "        consts.append(' '.join(tree.leaves()))\n",
    "        for subtree in tree:\n",
    "            c = constituents(subtree)\n",
    "            if type(c) is str:\n",
    "                consts.append(c)\n",
    "            else:\n",
    "                consts += c\n",
    "    else:\n",
    "        consts = tree[0]\n",
    "    return consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pparser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e2b769ced41f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# for tree in pparser.parse('A user writes'.split(), n=1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'A user writes something on paper with a pen'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconstituents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pparser' is not defined"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# for tree in pparser.parse('A user writes'.split(), n=1):\n",
    "for tree in pparser.parse('A user writes something on paper with a pen'.split(), n=2):\n",
    "    print(constituents(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseval_tree(gold, cand):\n",
    "    gold_consts = constituents(gold)\n",
    "    cand_consts = constituents(cand)\n",
    "    correct_cand = [c for c in cand_consts if c in gold_consts]\n",
    "    correct_gold = [c for c in gold_consts if c in cand_consts]\n",
    "    precision = len(correct_cand) / len(cand_consts)\n",
    "    recall = len(correct_gold) / len(gold_consts)\n",
    "    F = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, F\n",
    "    \n",
    "\n",
    "def parseval(gold_trees, parser):\n",
    "    ps, rs, Fs = [], [], []\n",
    "    # get values for each sentence\n",
    "    for gold_tree in gold_trees:\n",
    "        sentence = gold_tree.leaves()\n",
    "        if len(sentence) > 5: \n",
    "            # long sentences take ages to parse\n",
    "            continue\n",
    "        try:\n",
    "            cand_tree = next(parser.parse(sentence, n=1))\n",
    "            p, r, F = parseval_tree(gold_tree, cand_tree)\n",
    "        except StopIteration:\n",
    "            p, r, F = 0.0, 0.0, 0.0\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "        Fs.append(F)\n",
    "    # return averages\n",
    "    return [sum(v) / len(v) for v in [ps, rs, Fs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation time:  12.922845602035522 s\n",
      "\n",
      "average precision: 0.46927689594356276\n",
      "average recall: 0.5555555555555556\n",
      "average F-score: 0.508375230434054\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "gold_trees = filter(is_tree_valid, parse_treebank('en_lines-ud-dev.s'))\n",
    "pparser = PCKYParser(grammar)\n",
    "t = time()\n",
    "ap, ar, aF = parseval(gold_trees, pparser)\n",
    "\n",
    "print('evaluation time: ', time()-t, 's\\n')\n",
    "print('average precision:', ap)\n",
    "print('average recall:', ar)\n",
    "print('average F-score:', aF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Great!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Bonus* (20 points)\n",
    "\n",
    "Implement a class that converts Python-style regular expressions to XSLT-style ones, and executes them via foma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Conversion* (10 points)\n",
    "\n",
    "The functionality should be encapsulated in a class called `FomaRegex`. The public API specification is as follows:\n",
    "- its constructor should accept a valid Python regex string (not a [regex object](https://docs.python.org/3/library/re.html#regular-expression-objects)), convert it to the XFST format and store it in its `pattern` member field\n",
    "- it should have a `convert` static method that does the pattern conversion. You can use pure Python or better yet, a CFG grammar\n",
    "- the class should implement the context manager protocol:\n",
    "    - when entering the context, an FSA file should be created via foma and its name stored in the `fsa_file` field.  The [`regex <regex> ;` command](https://github.com/mhulden/foma/blob/master/foma/docs/simpleintro.md#compiling-regular-expressions) can be used to compile a regex in foma; for the rest, refer to the [`compile_lexc()` function](../../course_material/09_Morphology_lab/09_Morphology_lab_solutions.ipynb#Morphology)\n",
    "    - after the context closes, the FSA file should be deleted and the `fsa_file` member set to `None`\n",
    "\n",
    "You only need to account for the first six rows in [the table comparing the two syntaxes](../../course_material/08_Morphology/08_Morphology_lecture.ipynb#XFST-vs-Python-regular-expressions). Additionally, you only need to cover the characters a-zA-Z0-9 (i.e. no punctuation). Note that there are two options for verbatim texts in XFST: `[a b c]` or `{abc}`. You are encouraged to use the latter; should you choose to use the former, update the assert statements accordingly.\n",
    "\n",
    "You don't have to worry about applying the regex at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class FomaRegex:\n",
    "    pass\n",
    "\n",
    "# Tests\n",
    "assert FomaRegex.convert('ab?c*d+') == '{a}{b}^<2{c}*{d}+'\n",
    "assert FomaRegex.convert('a.b') == '{a}?{b}'\n",
    "assert FomaRegex.convert('a+(bc|de).*') == '{a}+[{bc}|{de}]?*'\n",
    "\n",
    "with FomaRegex('a.b') as fr:\n",
    "    assert fr.pattern == '{a}?{b}', 'Invalid pattern'\n",
    "    assert fr.fsa_file is not None, 'FSA file is None in with'\n",
    "    fsa_file = fr.fsa_file\n",
    "assert fr.fsa_file is None, 'FSA file is not None after with'\n",
    "assert not os.path.isfile(fsa_file), 'FSA file still exists after with'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Application* (5 points)\n",
    "\n",
    "Add a `match` method to the class that runs the regex against the specified string. It should return `True` or `False` depending on whether the regex matched the string.\n",
    "\n",
    "Note: obviously you should use your FSA file and foma, not the `re` module. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "with FomaRegex('a*(bc|de).+') as fr:\n",
    "    assert fr.match('aabcd') is True\n",
    "    assert fr.match('ade') is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multiple regexes (5 points)\n",
    "\n",
    "Make sure not all `FomaRegex` objects use the same FSA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "with FomaRegex('a') as a, FomaRegex('b') as b:\n",
    "    assert a.fsa_file != b.fsa_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
