{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Katona Máté (PD6YOR)\n",
    "\n",
    "# Homework 3\n",
    "\n",
    "The maximum score of this homework is 100+20 points. Grading is listed in this table:\n",
    "\n",
    "| Grade | Score range |\n",
    "| --- | --- |\n",
    "| 5 | 85+ |\n",
    "| 4 | 70-84 |\n",
    "| 3 | 55-69 |\n",
    "| 2 | 40-54 |\n",
    "| 1 | 0-39 |\n",
    "\n",
    "Most exercises include tests which should pass if your solution is correct.\n",
    "However, successful tests do not guarantee that your solution is correct.\n",
    "You are free to add more tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline\n",
    "\n",
    "Monday, 11 December 2017, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "\n",
    "Feel free to copy any boilerplate code you need from labs [9](../../course_material/09_Morphology_lab/09_Morphology_lab.ipynb#Morphology) and [10](../../course_material/10_Syntax/10_Syntax_lab.ipynb#Boilerplate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from functools import partial\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "def execute_commands(*cmds, fancy=True):\n",
    "    \"\"\"\n",
    "    Starts foma end executes the specified commands.\n",
    "    Might not work if there are too many...\n",
    "    \"\"\"\n",
    "    if fancy:\n",
    "        print('Executing commands...\\n=====================\\n')\n",
    "    args = ' '.join('-e \"{}\"'.format(cmd) for cmd in cmds)\n",
    "    output = subprocess.check_output('foma {} -s'.format(args),\n",
    "                                     stderr=subprocess.STDOUT,\n",
    "                                     shell=True).decode('utf-8')\n",
    "    print(output)\n",
    "    if fancy:\n",
    "        print('=====================\\n')\n",
    "    \n",
    "def compile_lexc(lexc_string, fst_file):\n",
    "    \"\"\"\n",
    "    Compiles a string describing a lexc lexicon with foma. The FST\n",
    "    is written to fst_file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='wt', encoding='utf-8', delete=False) as outf:\n",
    "            outf.write(lexc_string)\n",
    "    try:\n",
    "        execute_commands('read lexc {}'.format(outf.name),\n",
    "                         'save stack {}'.format(fst_file), fancy=False)\n",
    "        #!foma -e \"read lexc {outf.name}\" -e \"save stack {fst_file}\" -s\n",
    "    finally:\n",
    "        os.remove(outf.name)\n",
    "        \n",
    "def apply(fst_file, words, up=True):\n",
    "    \"\"\"\n",
    "    Applies the FST in fst_file on the supplied words. The default direction\n",
    "    is up.\n",
    "    \"\"\"\n",
    "    if isinstance(words, list):\n",
    "        words = '\\n'.join(map(str, words))\n",
    "    elif not isinstance(words, str):\n",
    "        raise ValueError('words must be a str or list')\n",
    "    header = 'Applying {} {}...'.format(fst_file, 'up' if up else 'down')\n",
    "    print('{}\\n{}\\n'.format(header, '=' * len(header)))\n",
    "    invert = '-i' if not up else ''\n",
    "    result = subprocess.check_output('flookup {} {}'.format(invert, fst_file),\n",
    "                                     stderr=subprocess.STDOUT, shell=True,\n",
    "                                     input=words.encode('utf-8'))\n",
    "    print(result.decode('utf-8')[:-1])  # Skip last newline\n",
    "    print('=' * len(header), '\\n')\n",
    "       \n",
    "apply_up = partial(apply, up=True)\n",
    "apply_down = partial(apply, up=False)\n",
    "\n",
    "def draw_net(fst_file, inline=True):\n",
    "    \"\"\"\n",
    "    Displays a compiled network inline or in a separate window.\n",
    "    The package imagemagic must be installed for this function to work.\n",
    "    \"\"\"\n",
    "    !foma -e \"load stack {fst_file}\" -e \"print dot >{fst_file}.dot\" -s\n",
    "    if inline:\n",
    "        png_data = subprocess.check_output(\n",
    "            'cat {}.dot | dot -Tpng'.format(fst_file), shell=True)\n",
    "        display(Image(data=png_data, format='png'))\n",
    "    else:\n",
    "        !cat {fst_file}.dot | dot -Tpng | display\n",
    "    !rm {fst_file}.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import nltk\n",
    "from nltk import Nonterminal\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def does_tcl_work():\n",
    "    \"\"\"Checks if Tcl is installed and works (e.g. it won't on a headless server).\"\"\"\n",
    "    tree = nltk.tree.Tree('test', [])\n",
    "    try:\n",
    "        tree._repr_png_()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def draw_tree(tree):\n",
    "    \"\"\"Draws an NLTK parse tree via Graphviz.\"\"\"\n",
    "    def draw_tree_rec(curr_root, graph, last_node):\n",
    "        node_id = str(int(last_node) + 1)\n",
    "        for child in curr_root:\n",
    "            if isinstance(child, nltk.tree.Tree):\n",
    "                graph.node(node_id, child.label(), penwidth='0')\n",
    "                graph.edge(last_node, node_id, color='darkslategray3', style='bold')\n",
    "                node_id = draw_tree_rec(child, graph, node_id)\n",
    "            else:\n",
    "                graph.node(node_id, child, penwidth='0')\n",
    "                graph.edge(last_node, node_id, color='darkslategray3', style='bold')\n",
    "                node_id = str(int(node_id) + 1)\n",
    "        return str(int(node_id) + 1)\n",
    "    \n",
    "    graph = graphviz.Graph()\n",
    "    graph.graph_attr['ranksep'] = '0.2'\n",
    "    graph.node('0', tree.label(), penwidth='0')\n",
    "    draw_tree_rec(tree, graph, '0')\n",
    "    return graph._repr_svg_()\n",
    "\n",
    "# Use Graphviz to draw the tree if the Tcl backend of nltk doesn't work\n",
    "if not does_tcl_work():\n",
    "    svg_formatter = get_ipython().display_formatter.formatters['image/svg+xml']\n",
    "    svg_formatter.for_type(nltk.tree.Tree, draw_tree)\n",
    "    # Delete the nltk drawing function, just to be sure\n",
    "    delattr(Tree, '_repr_png_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Morphology (45 points)\n",
    "\n",
    "\n",
    "## 1.1 Extend the lexc grammar (20 points)\n",
    "\n",
    "This exercise assumes that you have finished the lexc lab exercises. The grammar you hand in have to handle the adjectives listed in the lab exercises, as well as the number (singular / plural), the nominative and accusative cases, comparative and superlative forms, and vowel harmony.\n",
    "\n",
    "There are two sub-tasks to this exercise. You can download them from\n",
    "1. http://sandbox.mokk.bme.hu/~ndavid/homework3/{NEPTUN_CODE}/homework3_1_1_1.ipynb\n",
    "1. http://sandbox.mokk.bme.hu/~ndavid/homework3/{NEPTUN_CODE}/homework3_1_1_2.ipynb\n",
    "\n",
    "(Note: all letters in the Neptun code must be capitalized.)\n",
    "\n",
    "Please don't solve the exercises in the downloaded notebooks, but copy the descriptions and starter code snippets to the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Nouns\n",
    "\n",
    "Extend the grammar with nouns ending with a vowel.\n",
    "\n",
    "For the first group, the declension requires modification of the root: _**a**_ becomes _**á**_ and _**e**_ becomes _**é**_. For both groups, the linking vowel is the same as for adjectives.\n",
    "\n",
    "Examples:\n",
    "- _kutya_ + `[Pl]` $\\rightarrow$ _kuty**á**k_\n",
    "- _kutya_ + `[Acc]` $\\rightarrow$ _kuty**á**t_\n",
    "- _kutya_ + `[Pl]` + `[Acc]` $\\rightarrow$ _kuty**á**k**a**t_\n",
    "\n",
    "Hint: handle the inflected forms first and then find a shortcut for the `[Nom]` case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_1 = \"\"\"\n",
    "kutya ! dog\n",
    "szalma ! straw\n",
    "utca ! street\n",
    "nénike ! little old lady\n",
    "öntöde ! foundry\n",
    "pecsenye ! roast\n",
    "\"\"\"\n",
    "\n",
    "nouns_2 = \"\"\"\n",
    "néni ! old lady\n",
    "tű ! needle\n",
    "randevú ! date (back)\n",
    "szajré ! swag (inf, back)\n",
    "jeti ! yeti\n",
    "faodú ! tree hollow (back)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "Multichar_Symbols \n",
    "    ! legesleg[/Supl]piros[/Adj|col]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
    "    ! nénike[/N]k[Pl]et[Acc]\n",
    "    [/Supl] [/Adj] [_Cmp/Adj] [Pl] [Acc] [/N]\n",
    "    @U.HARM.F@ @U.HARM.B@  ! harmony\n",
    "    @P.EX.T@ @R.EX@ @D.EX@  ! exaggarative \n",
    "    @P.SUP.T@ @R.SUP@ @D.SUP@  ! superlative\n",
    "    @P.VOW.T@ @R.VOW@ @D.VOW@ @C.VOW@  ! vowel ending\n",
    "    @P.MOD.T@ @P.MOD.F@ @R.MOD.T@ @R.MOD.F@ @D.MOD.T@ @D.MOD.F@ @C.MOD@  ! root modification\n",
    "\n",
    "LEXICON Root\n",
    "    @P.VOW.T@     nouns ;  ! in this example, all nouns end with a vowel\n",
    "                  ex    ;  ! will continue to adjs\n",
    "\n",
    "LEXICON nouns\n",
    "    @U.HARM.B@    bnouns     ;\n",
    "    @U.HARM.B@    bnouns_mod ;\n",
    "    @U.HARM.F@    fnouns     ;\n",
    "    @U.HARM.F@    fnouns_mod ;\n",
    "\n",
    "LEXICON fnouns\n",
    "    néni          noun_tag ;\n",
    "    tű            noun_tag ;\n",
    "    jeti          noun_tag ;\n",
    "\n",
    "LEXICON bnouns\n",
    "    randevú       noun_tag ;\n",
    "    szajré        noun_tag ;\n",
    "    faodú         noun_tag ;\n",
    "\n",
    "LEXICON fnouns_mod\n",
    "    nénik         mod ;\n",
    "    öntöd         mod ;\n",
    "    pecseny       mod ;\n",
    "\n",
    "LEXICON bnouns_mod\n",
    "    kuty          mod ;\n",
    "    szalm         mod ;\n",
    "    utc           mod ;\n",
    "\n",
    "LEXICON mod\n",
    "    @U.HARM.B@@P.MOD.F@a:@U.HARM.B@@P.MOD.F@a noun_tag ;\n",
    "    @U.HARM.B@@P.MOD.T@a:@U.HARM.B@@P.MOD.T@á noun_tag ;\n",
    "    @U.HARM.F@@P.MOD.F@e:@U.HARM.F@@P.MOD.F@e noun_tag ;\n",
    "    @U.HARM.F@@P.MOD.T@e:@U.HARM.F@@P.MOD.T@é noun_tag ;\n",
    "\n",
    "LEXICON noun_tag \n",
    "    @R.MOD.F@[/N]:@R.MOD.F@0 # ;  ! only non-modified stems (MOD.F) can stop\n",
    "    @D.MOD.F@[/N]:@D.MOD.F@0 plur ;\n",
    "\n",
    "LEXICON ex\n",
    "    @P.EX.T@leges ex  ;\n",
    "                  sup ;\n",
    "\n",
    "LEXICON sup\n",
    "    @P.SUP.T@leg  sup_tag ;\n",
    "    @D.EX@        adjs ;\n",
    "\n",
    "LEXICON sup_tag\n",
    "    [/Supl]:0    adjs ;\n",
    "\n",
    "LEXICON adjs\n",
    "    @U.HARM.F@    fadjs ;\n",
    "    @U.HARM.B@    badjs ;\n",
    "\n",
    "LEXICON fadjs\n",
    "    csendes       adj_tag ;\n",
    "    egészséges    adj_tag ;\n",
    "    idős          adj_tag ;\n",
    "    kék           adj_tag ;\n",
    "    mély          adj_tag ;\n",
    "    öntelt        adj_tag ;\n",
    "    szeles        adj_tag ;\n",
    "    terhes        adj_tag ;\n",
    "    zsémbes       adj_tag ;\n",
    "\n",
    "LEXICON badjs\n",
    "    abszurd       adj_tag ;\n",
    "    bájos         adj_tag ;\n",
    "    finom         adj_tag ;\n",
    "    gyanús        adj_tag ;\n",
    "    okos          adj_tag ;\n",
    "    piros         adj_tag ;\n",
    "    száraz        adj_tag ;\n",
    "    zord          adj_tag ;\n",
    "\n",
    "LEXICON adj_tag\n",
    "    [/Adj]:0       comp ;\n",
    "\n",
    "LEXICON comp \n",
    "    @D.SUP@       plur;\n",
    "    @U.HARM.F@ebb comp_tag ;\n",
    "    @U.HARM.B@abb comp_tag ;\n",
    "\n",
    "LEXICON comp_tag\n",
    "    [_Comp/Adj]:0  plur ;\n",
    "\n",
    "LEXICON plur\n",
    "                                       case ;\n",
    "    @R.VOW@@C.VOW@@C.MOD@k             plur_tag ;  ! only with vowel-ending words, but overrides both vowel-ending (C.VOW) and modified stem (C.MOD)\n",
    "    @D.VOW@@U.HARM.F@ek                plur_tag ;\n",
    "    @D.VOW@@U.HARM.B@ak                plur_tag ;\n",
    "\n",
    "LEXICON plur_tag\n",
    "    [Pl]:0        case ;\n",
    "\n",
    "LEXICON case\n",
    "    @D.MOD.T@            # ;  ! modified stems are not allowed to stop\n",
    "    @R.VOW@@C.VOW@t      case_tag ;\n",
    "    @D.VOW@@U.HARM.F@et  case_tag ;\n",
    "    @D.VOW@@U.HARM.B@at  case_tag ;\n",
    "\n",
    "LEXICON case_tag\n",
    "    [Acc]:0       # ;\n",
    "\"\"\"\n",
    "\n",
    "compile_lexc(grammar, 'nouns.fst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_net('nouns.fst')\n",
    "apply_up('nouns.fst', ['nénikét', 'pecsenyék', 'tűket', 'faodú', 'legeslegeslegszárazabbakat'])        \n",
    "execute_commands('load stack nouns.fst', 'print upper-words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 The allative case\n",
    "\n",
    "Extend the grammar with the allative case. This case differs from accusative in two ways:\n",
    "- it never gets a linking vowel\n",
    "- it has three forms (_-hoz_ / _-hez_ / _-höz_). The first two are used for the words you would expect. The third one is used for words which takes front-vowel inflections, but whose last vowel is a _**ö**_ or _**ő**_ (yes, there is only one of those in the lab word lists).\n",
    "\n",
    "Examples:\n",
    "- _finom_ + `[All]` $\\rightarrow$ _finom**hoz**_\n",
    "- _mély_ + `[All]` $\\rightarrow$ _mély**hez**_\n",
    "- _idős_ + `[All]` $\\rightarrow$ _idős**höz**_\n",
    "\n",
    "Also add the following adjectives to the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = \"\"\"\n",
    "bűnös ! sinful\n",
    "ösztönös ! instinctive\n",
    "felnőtt ! adult\n",
    "dühös ! angry\n",
    "erős ! strong\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "Multichar_Symbols \n",
    "    ! erős[/Adj]höz[All]\t\n",
    "    [/Supl] [/Adj] [_Comp/Adj] [Pl] [Acc]\n",
    "    @P.HARM.F@ @P.HARM.B@ @P.HARM.O@ @D.HARM.B@ @R.HARM.F@ @R.HARM.B@ @R.HARM.O@  ! harmony\n",
    "    @P.EX.T@ @R.EX@ @D.EX@  ! exaggarative\n",
    "    @P.SUP.T@ @R.SUP@ @D.SUP@  ! superlative\n",
    "\n",
    "LEXICON Root\n",
    "                  ex    ;  ! will continue to adjs\n",
    "\n",
    "LEXICON ex\n",
    "    @P.EX.T@leges ex  ;\n",
    "                  sup ;\n",
    "\n",
    "LEXICON sup\n",
    "    @P.SUP.T@leg  sup_tag ;\n",
    "    @D.EX@        adjs ;\n",
    "\n",
    "LEXICON sup_tag\n",
    "    [/Supl]:0    adjs ;\n",
    "\n",
    "LEXICON adjs\n",
    "    @P.HARM.F@    fadjs ;\n",
    "    @P.HARM.B@    badjs ;\n",
    "    @P.HARM.O@    oadjs ;\n",
    "\n",
    "LEXICON fadjs\n",
    "    csendes       adj_tag ;\n",
    "    egészséges    adj_tag ;\n",
    "!    idős          adj_tag ;  ! moved to oeadjs\n",
    "    kék           adj_tag ;\n",
    "    mély          adj_tag ;\n",
    "    öntelt        adj_tag ;\n",
    "    szeles        adj_tag ;\n",
    "    terhes        adj_tag ;\n",
    "    zsémbes       adj_tag ;\n",
    "\n",
    "LEXICON badjs\n",
    "    abszurd       adj_tag ;\n",
    "    bájos         adj_tag ;\n",
    "    finom         adj_tag ;\n",
    "    gyanús        adj_tag ;\n",
    "    okos          adj_tag ;\n",
    "    piros         adj_tag ;\n",
    "    száraz        adj_tag ;\n",
    "    zord          adj_tag ;\n",
    "\n",
    "LEXICON oadjs\n",
    "    idős          adj_tag ;\n",
    "    bűnös         adj_tag ;\n",
    "    ösztönös      adj_tag ;\n",
    "    felnőtt       adj_tag ;\n",
    "    dühös         adj_tag ;\n",
    "    erős          adj_tag ;\n",
    "\n",
    "LEXICON adj_tag\n",
    "    [/Adj]:0       comp ;\n",
    "\n",
    "LEXICON comp \n",
    "    @D.SUP@                 plur;\n",
    "    @D.HARM.B@@P.HARM.F@ebb comp_tag ;\n",
    "    @R.HARM.B@abb           comp_tag ;\n",
    "\n",
    "LEXICON comp_tag\n",
    "    [_Comp/Adj]:0  plur ;\n",
    "\n",
    "LEXICON plur\n",
    "                                case ;\n",
    "    @D.HARM.B@@P.HARM.F@ek      plur_tag ;\n",
    "    @R.HARM.B@ak                plur_tag ;\n",
    "\n",
    "LEXICON plur_tag\n",
    "    [Pl]:0        case ;\n",
    "\n",
    "LEXICON case\n",
    "                   # ;\n",
    "    @R.HARM.F@hez  all_tag ;\n",
    "    @R.HARM.B@hoz  all_tag ;\n",
    "    @R.HARM.O@höz  all_tag ;\n",
    "    @D.HARM.B@et   acc_tag ;\n",
    "    @R.HARM.B@at   acc_tag ;\n",
    "\n",
    "LEXICON acc_tag\n",
    "    [Acc]:0       # ;\n",
    "\n",
    "LEXICON all_tag\n",
    "    [All]:0       # ;\n",
    "\"\"\"\n",
    "\n",
    "compile_lexc(grammar, 'allative.fst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_net('allative.fst')\n",
    "apply_up('allative.fst', ['erősebbekhez', 'erőshöz', 'legeslegeslegpirosabbhoz', 'kéket'])        \n",
    "execute_commands('load stack allative.fst', 'print upper-words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CFG morphology (25 points)\n",
    "\n",
    "### 1.2.1 The basics (15 points)\n",
    "\n",
    "Implement morphological analysis with a CFG grammar. Requirements:\n",
    "- the grammar should handle everything we covered in the lab, except for vowel harmony\n",
    "- there is no need to handle generation; concentrate on parsing\n",
    "- it doesn't matter what the intermediate nonterminals are called. Ideally, _preterminals_ should be valid morphological tags, as in the example below; however, `nlkt.CFG.fromstring()` cannot parse tags such as `[Acc]` as nonterminals. You are free to call them whatever you wish, but descriptive names are encouraged.\n",
    "- encapsulate the functionality in a `CFGMorphParser` class:\n",
    "    - its `__init__()` should accept no parameters (or at least provide defaults)\n",
    "    - it should have a `parse_tree()` method, which accepts a word and returns the parse tree\n",
    "    - it should have a `parse()` method, which accepts a word and returns the morphological parse _in the same form as HFST_. Refer to `hfst_lookup` and the tests for the format. `nltk.tree.Tree.pos()` is a good starting point\n",
    "\n",
    "Note that having the same format as HFST doesn't mean you have to return the exact same output: for instance, we defined _terhes_ as a genuine adjective, even though it is derived from the noun _teher_. So HFST would analyze it as `teher[/N]es[_Adjz:s/Adj][Nom]`, but you only need to return `terhes[/Adj]`. You **also don't have to cover [Nom]**, because of [this bug](https://github.com/nltk/nltk/issues/1890)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "tree = Tree('__NOM_1', [\n",
    "    Tree('__NOM_2', [\n",
    "        Tree('__SUPL_ADJ_2', [\n",
    "            Tree('[Supl]', ['leg']),\n",
    "            Tree('[/Adj]', ['nagy']),\n",
    "            Tree('[_Comp/Adj]', ['obb']),\n",
    "        ]),\n",
    "        Tree('[Pl]', ['ak']),\n",
    "    ]),\n",
    "    Tree('[Acc]', ['at']),\n",
    "])\n",
    "\n",
    "display(tree)\n",
    "# If display() doesn't work, try this\n",
    "# tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here\n",
    "\n",
    "class CFGMorphParser(object):\n",
    "    def __init__(self):\n",
    "        self.grammar = nltk.CFG.fromstring(\"\"\"\n",
    "        __Nom -> __Nom2 __Acc\n",
    "        __Nom2 -> __SuplAdj __Pl\n",
    "        __SuplAdj -> __Supl __Adj __Comp | __Adj __Comp | __Adj\n",
    "        __Supl -> __Ex __Sup | __Sup\n",
    "        __Ex -> 'l' 'e' 'g' 'e' 's'\n",
    "        __Sup -> 'l' 'e' 'g'\n",
    "        __Adj -> 't' 'e' 'r' 'h' 'e' 's' | 'c' 's' 'e' 'n' 'd' 'e' 's' | 'f' 'i' 'n' 'o' 'm'\n",
    "        __Comp -> 'a' 'b' 'b' | 'e' 'b' 'b'\n",
    "        __Pl -> 'a' 'k' | 'e' 'k'\n",
    "        __Acc -> 'a' 't' | 'e' 't'\n",
    "        \"\"\")\n",
    "        self.p = nltk.ChartParser(self.grammar)\n",
    "        \n",
    "    def parse(self, word):\n",
    "        try:\n",
    "            trees = self.p.parse([char for char in word])\n",
    "        except ValueError:\n",
    "            return None\n",
    "        for tree in trees:\n",
    "            print(tree)\n",
    "            display(tree)\n",
    "            return ''.join(tree.leaves())\n",
    "    \n",
    "    \n",
    "    def parse_tree(self, word):\n",
    "        return Tree(word, [word])\n",
    "\n",
    "    # nltk.tree.Tree.pos\n",
    "    def pos(self):\n",
    "        pos = []\n",
    "        for child in self:\n",
    "            if isinstance(child, Tree):\n",
    "                pos.extend(child.pos())\n",
    "            else:\n",
    "                pos.append((child, self._label))\n",
    "        return pos\n",
    "\n",
    "    \n",
    "# Tests\n",
    "parser = CFGMorphParser()\n",
    "\n",
    "print(parser.parse('unknown_word'))\n",
    "print(parser.parse('terhes'))\n",
    "print(parser.parse('csendesebbet'))\n",
    "print(parser.parse('legfinomabbak'))\n",
    "print(parser.parse('legfinomabbek'))\n",
    "\n",
    "assert parser.parse('unknown_word') is None\n",
    "assert parser.parse('terhes') == 'terhes[/Adj]'\n",
    "assert parser.parse('csendesebbet') == 'csendes[/Adj]ebb[_Comp/Adj]et[Acc]'\n",
    "assert parser.parse('legfinomabbak') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ak[Pl]'\n",
    "# It's OK here\n",
    "assert parser.parse('legfinomabbek') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ek[Pl]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Vowel harmony (10 points)\n",
    "\n",
    "Also handle vowel harmony. Write a function that traverses the tree manually (similarly to [exercise 2.4 in the lab](../../course_material/10_Syntax/10_Syntax_lab_solutions.ipynb#2.4-Evaluation*)) and returns `True` or `False`, depending on whether the tree conforms to vowel harmony rules. Use this function in `parse_tree` (and `parse`) to filter invalid trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert parser.parse('legfinomabbak') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ak[Pl]'\n",
    "assert parser.parse('legfinomabbek') == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Syntax (55 points)\n",
    "\n",
    "In this exercise, you will parse a treebank, and induce a PCFG grammar from it. You will then implement a probabilistic version of the CKY algorithm, and evaluate the grammar on the test split of the treebank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Parse a treebank (10 points)\n",
    "\n",
    "Parse the treebank file `en_lines-ud-train.s` in the notebook's directory. Write a **generator** function that reads the file and yields `nltk.tree.Tree` objects. In particular,\n",
    "- do not read the whole file into memory\n",
    "- the `Tree.fromstring()` function converts an s-expression into a tree\n",
    "\n",
    "Open the file in an editor to see the formatting.\n",
    "\n",
    "Note that the file was created by parsing the [LinES dependency corpus](https://github.com/UniversalDependencies/UD_English-LinES/tree/master) with [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/), so it is not a gold standard by any means, but it will suffice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "\n",
    "def parse_treebank(treebank_file):\n",
    "    with open(treebank_file, 'r') as f:\n",
    "        s = ''\n",
    "        for l in f:\n",
    "            if len(l) > 1:\n",
    "                s += l\n",
    "            elif s:\n",
    "                try:\n",
    "                    t = Tree.fromstring(s)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                finally:\n",
    "                    s = ''\n",
    "                yield t\n",
    "            else:\n",
    "                s = ''\n",
    "            \n",
    "# Tests\n",
    "assert sum(1 for _ in parse_treebank('en_lines-ud-train.s')) == 2613\n",
    "assert isinstance(next(parse_treebank('en_lines-ud-train.s')), Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filter trees (5 points)\n",
    "\n",
    "In order to avoid problems further down the line, we shall only handle a subset of the trees in the treebank. We call a tree _valid_, if\n",
    "- its root is `'S'`\n",
    "- the root has at least two children.\n",
    "\n",
    "Write a function that returns `True` for \"valid\" trees and `False` for invalid ones. Filter the your generator with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tree_valid(tree):\n",
    "    if not isinstance(tree, Tree):\n",
    "        return False\n",
    "    if tree.label() != 'S':\n",
    "        return False\n",
    "    if len(tree) < 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Tests\n",
    "assert sum(map(is_tree_valid, parse_treebank('en_lines-ud-train.s'))) == 2311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Induce the PCFG grammar (10 points)\n",
    "\n",
    "Now that you have the trees, it is time to induce (train) a PCFG grammar for it! Luckily, `nltk` has a functions for just that: [`nltk.grammar.induce_pcfg`](http://www.nltk.org/api/nltk.html#nltk.grammar.induce_pcfg). Use it to acquire your PCFG grammar. You can find hints at how to use it in the [grammar module](http://www.nltk.org/_modules/nltk/grammar.html).\n",
    "\n",
    "Note: since we want to parse sentences with the PCKY algorithm, we need our grammar to be in CNF. Unfortunately, `nlkt` cannot convert a grammar to CNF, so you have to ensure that the trees are in CNF before feeding them to the PCFG induction function. That way, we can be sure that our grammar will be also. There are two functions that ensure a tree is in CNF:\n",
    "- [`collapse_unary`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.collapse_unary). Make sure you call it with `collapsePOS=True`!\n",
    "- [`chomsky_normal_form`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.chomsky_normal_form). Do not use any smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grammar(trees):\n",
    "    prods = []\n",
    "    for tree in trees: \n",
    "        tree.collapse_unary(collapsePOS=True)\n",
    "        tree.chomsky_normal_form()\n",
    "        prods += tree.productions()\n",
    "    return nltk.grammar.induce_pcfg(nltk.Nonterminal('S'), prods)\n",
    "        \n",
    "def is_grammar_cnf(grammar):\n",
    "    for prod in grammar.productions():\n",
    "        rhs = prod.rhs()\n",
    "        if len(rhs) > 2 or (len(rhs) == 1 and isinstance(rhs[0], nltk.Nonterminal)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Tests\n",
    "grammar = train_grammar(filter(is_tree_valid, parse_treebank('en_lines-ud-train.s')))\n",
    "assert is_grammar_cnf(grammar)\n",
    "assert len(grammar.productions()) == 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Implement PCKY (15 points)\n",
    "\n",
    "Implement the PCKY algorithm. Encapsulate it in a class called `PCKYParser`. Extend your `CKYParser` solution from the lab so that it creates trees with probabilities (`ProbabilisticTree`). The `parse()` method should also accept a parameter `n`, and only return the most probable `n` trees (as a generator).\n",
    "\n",
    "Some pointers:\n",
    "- [ProbabilisticTree](http://www.nltk.org/api/nltk.html#nltk.tree.ProbabilisticTree), which inherits from\n",
    "- [ProbabilisticMixIn](http://www.nltk.org/api/nltk.html#nltk.probability.ProbabilisticMixIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tree import ProbabilisticTree as PTree\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class PCKYParser(object):\n",
    "    def __init__(self, grammar):\n",
    "        if not isinstance(grammar, nltk.CFG):\n",
    "            raise TypeError(\"g\")\n",
    "        self.grammar = grammar\n",
    "    \n",
    "    def parse(self, sent, n=1):\n",
    "        k = len(sent)\n",
    "        # init\n",
    "        cky = np.empty((k,k), dtype=object)\n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                cky[i, j] = []\n",
    "        # lexical rules\n",
    "        for i, word in enumerate(sent):\n",
    "            for prod in self.grammar.productions():\n",
    "                if word in prod.rhs():\n",
    "                    cky[i, i].append(PTree(prod.lhs(), [word], prob=prod.prob()))\n",
    "        # production rules\n",
    "        for col in range(1, k):\n",
    "            for row in range(col-1, -1, -1):\n",
    "                ways_to_split = col-row  # \"distance\" from diag\n",
    "                for w in range(ways_to_split):\n",
    "                    left = ways_to_split - w\n",
    "                    down = 1 + w\n",
    "                    for prod in self.grammar.productions():\n",
    "                        if len(prod.rhs()) < 2:\n",
    "                            continue  # skip lexical productions\n",
    "                        # because of CNF, all non-lexical rules have 2 constituents on rhs\n",
    "                        ls = prod.rhs()[0]  # left symbol\n",
    "                        ds = prod.rhs()[1]  # down symbol\n",
    "                        for lt in cky[row, col-left]:  # left tree\n",
    "                            for dt in cky[row+down, col]:  # right tree\n",
    "                                if lt.label() == ls and dt.label() == ds:\n",
    "                                    # multiply tree and production probabilities\n",
    "                                    p = lt.prob() * dt.prob() * prod.prob()\n",
    "                                    cky[row, col].append(PTree(prod.lhs(), [lt, dt], prob=p))\n",
    "        # collect sentences from parsed trees, sort them by probability\n",
    "        sents = sorted(\n",
    "            [tree for tree in cky[0, k-1] if tree.label() == nltk.Nonterminal('S')],\n",
    "            key=lambda tree: tree.prob()\n",
    "        )\n",
    "        # yield the most probable n trees\n",
    "        for sent in deque(sents, maxlen=n):\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = train_grammar(filter(is_tree_valid, parse_treebank('en_lines-ud-train.s')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all trees:\n",
      "(S (NP (NN A) (NN user)) (VP+VBZ writes)) (p=1.36211e-13)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "ProbabilisticTree(S, [ProbabilisticTree(NP, [ProbabilisticTree(NN, ['A']) (p=0.0005569996286669142), ProbabilisticTree(NN, ['user']) (p=0.0014853323431117712)]) (p=4.914493515617823e-09), ProbabilisticTree(VP+VBZ, ['writes']) (p=0.03125)]) (p=1.3621101761690197e-13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DT A) (NN user)) (VP+VBZ writes)) (p=8.74454e-11)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "ProbabilisticTree(S, [ProbabilisticTree(NP, [ProbabilisticTree(DT, ['A']) (p=0.010554682236694363), ProbabilisticTree(NN, ['user']) (p=0.0014853323431117712)]) (p=3.155030665039823e-06), ProbabilisticTree(VP+VBZ, ['writes']) (p=0.03125)]) (p=8.744541754544963e-11)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "most probable tree:\n",
      "(S (NP (DT A) (NN user)) (VP+VBZ writes)) (p=8.74454e-11)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "ProbabilisticTree(S, [ProbabilisticTree(NP, [ProbabilisticTree(DT, ['A']) (p=0.010554682236694363), ProbabilisticTree(NN, ['user']) (p=0.0014853323431117712)]) (p=3.155030665039823e-06), ProbabilisticTree(VP+VBZ, ['writes']) (p=0.03125)]) (p=8.744541754544963e-11)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tests\n",
    "pparser = PCKYParser(grammar)\n",
    "trees = pparser.parse('A user writes'.split(), n=3)\n",
    "print('all trees:')\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    display(tree)\n",
    "print('-'*80)\n",
    "# let's see if it returns the most probable tree when n=1\n",
    "trees = pparser.parse('A user writes'.split(), n=1)\n",
    "print('most probable tree:')\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Evaluate the grammar (15 points)\n",
    "\n",
    "Evaluate your grammar on the test split of the treebank (`en_lines-ud-dev.s`). Implement the **unlabelled** PARSEVAL metric. See [the first answer for an example](https://linguistics.stackexchange.com/questions/1873/is-there-a-well-established-metric-to-measure-the-effectiveness-of-a-parsing-alg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituents(tree):\n",
    "    consts = []\n",
    "    if isinstance(tree, Tree) and isinstance(tree[0], Tree):\n",
    "        consts.append(' '.join(tree.leaves()))\n",
    "        for subtree in tree:\n",
    "            c = constituents(subtree)\n",
    "            if type(c) is str:\n",
    "                consts.append(c)\n",
    "            else:\n",
    "                consts += c\n",
    "    else:\n",
    "        consts = tree[0]\n",
    "    return consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "# test\n",
    "for tree in pparser.parse('A user writes'.split(), n=1):\n",
    "    print(constituents(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseval_tree(gold, cand):\n",
    "    gold_consts = constituents(gold)\n",
    "    cand_consts = constituents(cand)\n",
    "    correct_cand = [c for c in cand_consts if c in gold_consts]\n",
    "    correct_gold = [c for c in gold_consts if c in cand_consts]\n",
    "    precision = len(correct_cand) / len(cand_consts)\n",
    "    recall = len(correct_gold) / len(gold_consts)\n",
    "    F = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, F\n",
    "    \n",
    "\n",
    "def parseval(gold_trees, parser):\n",
    "    precisions, recalls, Fs = [], [], []\n",
    "    # get values for each sentence\n",
    "    for gold_tree in gold_trees:\n",
    "        sentence = gold_tree.leaves()\n",
    "        if len(sentence) > 10: continue\n",
    "        print('evaluating sentece:\\n\\t', ' '.join(sentence))\n",
    "        try:\n",
    "            cand_tree = next(parser.parse(sentence, n=1))  # TODO better solution?\n",
    "            p, r, F = parseval_tree(gold_tree, cand_tree)\n",
    "        except StopIteration:\n",
    "            p, r, F = 0.0, 0.0, 0.0\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        Fs.append(F)\n",
    "    # return averages\n",
    "    return [sum(v) / len(v) for v in [precisions, recalls, Fs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pparser = PCKYParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
     ]
    }
   ],
   "source": [
    "gold_trees = filter(is_tree_valid, parse_treebank('en_lines-ud-dev.s'))\n",
    "\n",
    "ap, ar, aF = parseval(gold_trees, pparser)\n",
    "\n",
    "print('-'*90)\n",
    "print('average precision:', ap)\n",
    "print('average recall:', ar)\n",
    "print('average F-score:', aF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Bonus* (20 points)\n",
    "\n",
    "Implement a class that converts Python-style regular expressions to XSLT-style ones, and executes them via foma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Conversion* (10 points)\n",
    "\n",
    "The functionality should be encapsulated in a class called `FomaRegex`. The public API specification is as follows:\n",
    "- its constructor should accept a valid Python regex string (not a [regex object](https://docs.python.org/3/library/re.html#regular-expression-objects)), convert it to the XFST format and store it in its `pattern` member field\n",
    "- it should have a `convert` static method that does the pattern conversion. You can use pure Python or better yet, a CFG grammar\n",
    "- the class should implement the context manager protocol:\n",
    "    - when entering the context, an FSA file should be created via foma and its name stored in the `fsa_file` field.  The [`regex <regex> ;` command](https://github.com/mhulden/foma/blob/master/foma/docs/simpleintro.md#compiling-regular-expressions) can be used to compile a regex in foma; for the rest, refer to the [`compile_lexc()` function](../../course_material/09_Morphology_lab/09_Morphology_lab_solutions.ipynb#Morphology)\n",
    "    - after the context closes, the FSA file should be deleted and the `fsa_file` member set to `None`\n",
    "\n",
    "You only need to account for the first six rows in [the table comparing the two syntaxes](../../course_material/08_Morphology/08_Morphology_lecture.ipynb#XFST-vs-Python-regular-expressions). Additionally, you only need to cover the characters a-zA-Z0-9 (i.e. no punctuation). Note that there are two options for verbatim texts in XFST: `[a b c]` or `{abc}`. You are encouraged to use the latter; should you choose to use the former, update the assert statements accordingly.\n",
    "\n",
    "You don't have to worry about applying the regex at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class FomaRegex:\n",
    "    pass\n",
    "\n",
    "# Tests\n",
    "assert FomaRegex.convert('ab?c*d+') == '{a}{b}^<2{c}*{d}+'\n",
    "assert FomaRegex.convert('a.b') == '{a}?{b}'\n",
    "assert FomaRegex.convert('a+(bc|de).*') == '{a}+[{bc}|{de}]?*'\n",
    "\n",
    "with FomaRegex('a.b') as fr:\n",
    "    assert fr.pattern == '{a}?{b}', 'Invalid pattern'\n",
    "    assert fr.fsa_file is not None, 'FSA file is None in with'\n",
    "    fsa_file = fr.fsa_file\n",
    "assert fr.fsa_file is None, 'FSA file is not None after with'\n",
    "assert not os.path.isfile(fsa_file), 'FSA file still exists after with'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Application* (5 points)\n",
    "\n",
    "Add a `match` method to the class that runs the regex against the specified string. It should return `True` or `False` depending on whether the regex matched the string.\n",
    "\n",
    "Note: obviously you should use your FSA file and foma, not the `re` module. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "with FomaRegex('a*(bc|de).+') as fr:\n",
    "    assert fr.match('aabcd') is True\n",
    "    assert fr.match('ade') is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multiple regexes (5 points)\n",
    "\n",
    "Make sure not all `FomaRegex` objects use the same FSA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "with FomaRegex('a') as a, FomaRegex('b') as b:\n",
    "    assert a.fsa_file != b.fsa_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
