{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Katona Máté (PD6YOR)\n",
    "\n",
    "# Homework 3\n",
    "\n",
    "The maximum score of this homework is 100+20 points. Grading is listed in this table:\n",
    "\n",
    "| Grade | Score range |\n",
    "| --- | --- |\n",
    "| 5 | 85+ |\n",
    "| 4 | 70-84 |\n",
    "| 3 | 55-69 |\n",
    "| 2 | 40-54 |\n",
    "| 1 | 0-39 |\n",
    "\n",
    "Most exercises include tests which should pass if your solution is correct.\n",
    "However, successful tests do not guarantee that your solution is correct.\n",
    "You are free to add more tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline\n",
    "\n",
    "Monday, 11 December 2017, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "\n",
    "Feel free to copy any boilerplate code you need from labs [9](../../course_material/09_Morphology_lab/09_Morphology_lab.ipynb#Morphology) and [10](../../course_material/10_Syntax/10_Syntax_lab.ipynb#Boilerplate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from functools import partial\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "def execute_commands(*cmds, fancy=True):\n",
    "    \"\"\"\n",
    "    Starts foma end executes the specified commands.\n",
    "    Might not work if there are too many...\n",
    "    \"\"\"\n",
    "    if fancy:\n",
    "        print('Executing commands...\\n=====================\\n')\n",
    "    args = ' '.join('-e \"{}\"'.format(cmd) for cmd in cmds)\n",
    "    output = subprocess.check_output('foma {} -s'.format(args),\n",
    "                                     stderr=subprocess.STDOUT,\n",
    "                                     shell=True).decode('utf-8')\n",
    "    print(output)\n",
    "    if fancy:\n",
    "        print('=====================\\n')\n",
    "    \n",
    "def compile_lexc(lexc_string, fst_file):\n",
    "    \"\"\"\n",
    "    Compiles a string describing a lexc lexicon with foma. The FST\n",
    "    is written to fst_file.\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='wt', encoding='utf-8', delete=False) as outf:\n",
    "            outf.write(lexc_string)\n",
    "    try:\n",
    "        execute_commands('read lexc {}'.format(outf.name),\n",
    "                         'save stack {}'.format(fst_file), fancy=False)\n",
    "        #!foma -e \"read lexc {outf.name}\" -e \"save stack {fst_file}\" -s\n",
    "    finally:\n",
    "        os.remove(outf.name)\n",
    "        \n",
    "def apply(fst_file, words, up=True):\n",
    "    \"\"\"\n",
    "    Applies the FST in fst_file on the supplied words. The default direction\n",
    "    is up.\n",
    "    \"\"\"\n",
    "    if isinstance(words, list):\n",
    "        words = '\\n'.join(map(str, words))\n",
    "    elif not isinstance(words, str):\n",
    "        raise ValueError('words must be a str or list')\n",
    "    header = 'Applying {} {}...'.format(fst_file, 'up' if up else 'down')\n",
    "    print('{}\\n{}\\n'.format(header, '=' * len(header)))\n",
    "    invert = '-i' if not up else ''\n",
    "    result = subprocess.check_output('flookup {} {}'.format(invert, fst_file),\n",
    "                                     stderr=subprocess.STDOUT, shell=True,\n",
    "                                     input=words.encode('utf-8'))\n",
    "    print(result.decode('utf-8')[:-1])  # Skip last newline\n",
    "    print('=' * len(header), '\\n')\n",
    "       \n",
    "apply_up = partial(apply, up=True)\n",
    "apply_down = partial(apply, up=False)\n",
    "\n",
    "def draw_net(fst_file, inline=True):\n",
    "    \"\"\"\n",
    "    Displays a compiled network inline or in a separate window.\n",
    "    The package imagemagic must be installed for this function to work.\n",
    "    \"\"\"\n",
    "    !foma -e \"load stack {fst_file}\" -e \"print dot >{fst_file}.dot\" -s\n",
    "    if inline:\n",
    "        png_data = subprocess.check_output(\n",
    "            'cat {}.dot | dot -Tpng'.format(fst_file), shell=True)\n",
    "        display(Image(data=png_data, format='png'))\n",
    "    else:\n",
    "        !cat {fst_file}.dot | dot -Tpng | display\n",
    "    !rm {fst_file}.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import nltk\n",
    "from nltk import Nonterminal\n",
    "from nltk.parse.generate import generate\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def does_tcl_work():\n",
    "    \"\"\"Checks if Tcl is installed and works (e.g. it won't on a headless server).\"\"\"\n",
    "    tree = nltk.tree.Tree('test', [])\n",
    "    try:\n",
    "        tree._repr_png_()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def draw_tree(tree):\n",
    "    \"\"\"Draws an NLTK parse tree via Graphviz.\"\"\"\n",
    "    def draw_tree_rec(curr_root, graph, last_node):\n",
    "        node_id = str(int(last_node) + 1)\n",
    "        for child in curr_root:\n",
    "            if isinstance(child, nltk.tree.Tree):\n",
    "                graph.node(node_id, child.label(), penwidth='0')\n",
    "                graph.edge(last_node, node_id, color='darkslategray3', style='bold')\n",
    "                node_id = draw_tree_rec(child, graph, node_id)\n",
    "            else:\n",
    "                graph.node(node_id, child, penwidth='0')\n",
    "                graph.edge(last_node, node_id, color='darkslategray3', style='bold')\n",
    "                node_id = str(int(node_id) + 1)\n",
    "        return str(int(node_id) + 1)\n",
    "    \n",
    "    graph = graphviz.Graph()\n",
    "    graph.graph_attr['ranksep'] = '0.2'\n",
    "    graph.node('0', tree.label(), penwidth='0')\n",
    "    draw_tree_rec(tree, graph, '0')\n",
    "    return graph._repr_svg_()\n",
    "\n",
    "# Use Graphviz to draw the tree if the Tcl backend of nltk doesn't work\n",
    "if not does_tcl_work():\n",
    "    svg_formatter = get_ipython().display_formatter.formatters['image/svg+xml']\n",
    "    svg_formatter.for_type(nltk.tree.Tree, draw_tree)\n",
    "    # Delete the nltk drawing function, just to be sure\n",
    "    delattr(Tree, '_repr_png_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Morphology (45 points)\n",
    "\n",
    "\n",
    "## 1.1 Extend the lexc grammar (20 points)\n",
    "\n",
    "This exercise assumes that you have finished the lexc lab exercises. The grammar you hand in have to handle the adjectives listed in the lab exercises, as well as the number (singular / plural), the nominative and accusative cases, comparative and superlative forms, and vowel harmony.\n",
    "\n",
    "There are two sub-tasks to this exercise. You can download them from\n",
    "1. http://sandbox.mokk.bme.hu/~ndavid/homework3/{NEPTUN_CODE}/homework3_1_1_1.ipynb\n",
    "1. http://sandbox.mokk.bme.hu/~ndavid/homework3/{NEPTUN_CODE}/homework3_1_1_2.ipynb\n",
    "\n",
    "(Note: all letters in the Neptun code must be capitalized.)\n",
    "\n",
    "Please don't solve the exercises in the downloaded notebooks, but copy the descriptions and starter code snippets to the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Nouns\n",
    "\n",
    "Extend the grammar with nouns ending with a vowel.\n",
    "\n",
    "For the first group, the declension requires modification of the root: _**a**_ becomes _**á**_ and _**e**_ becomes _**é**_. For both groups, the linking vowel is the same as for adjectives.\n",
    "\n",
    "Examples:\n",
    "- _kutya_ + `[Pl]` $\\rightarrow$ _kuty**á**k_\n",
    "- _kutya_ + `[Acc]` $\\rightarrow$ _kuty**á**t_\n",
    "- _kutya_ + `[Pl]` + `[Acc]` $\\rightarrow$ _kuty**á**k**a**t_\n",
    "\n",
    "Hint: handle the inflected forms first and then find a shortcut for the `[Nom]` case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_1 = \"\"\"\n",
    "kutya ! dog\n",
    "szalma ! straw\n",
    "utca ! street\n",
    "nénike ! little old lady\n",
    "öntöde ! foundry\n",
    "pecsenye ! roast\n",
    "\"\"\"\n",
    "\n",
    "nouns_2 = \"\"\"\n",
    "néni ! old lady\n",
    "tű ! needle\n",
    "randevú ! date (back)\n",
    "szajré ! swag (inf, back)\n",
    "jeti ! yeti\n",
    "faodú ! tree hollow (back)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "Multichar_Symbols \n",
    "    ! legesleg[/Supl]piros[/Adj|col]abb[_Comp/Adj]ak[Pl]at[Acc]\n",
    "    ! nénike[/N]k[Pl]et[Acc]\n",
    "    [/Supl] [/Adj] [_Cmp/Adj] [Pl] [Acc] [/N]\n",
    "    @U.HARM.F@ @U.HARM.B@  ! harmony\n",
    "    @P.EX.T@ @R.EX@ @D.EX@  ! exaggarative \n",
    "    @P.SUP.T@ @R.SUP@ @D.SUP@  ! superlative\n",
    "    @P.VOW.T@ @R.VOW@ @D.VOW@ @C.VOW@  ! vowel ending\n",
    "    @P.MOD.T@ @P.MOD.F@ @R.MOD.T@ @R.MOD.F@ @D.MOD.T@ @D.MOD.F@ @C.MOD@  ! root modification\n",
    "\n",
    "LEXICON Root\n",
    "    @P.VOW.T@     nouns ;  ! in this example, all nouns end with a vowel\n",
    "                  ex    ;  ! will continue to adjs\n",
    "\n",
    "LEXICON nouns\n",
    "    @U.HARM.B@    bnouns     ;\n",
    "    @U.HARM.B@    bnouns_mod ;\n",
    "    @U.HARM.F@    fnouns     ;\n",
    "    @U.HARM.F@    fnouns_mod ;\n",
    "\n",
    "LEXICON fnouns\n",
    "    néni          noun_tag ;\n",
    "    tű            noun_tag ;\n",
    "    jeti          noun_tag ;\n",
    "\n",
    "LEXICON bnouns\n",
    "    randevú       noun_tag ;\n",
    "    szajré        noun_tag ;\n",
    "    faodú         noun_tag ;\n",
    "\n",
    "LEXICON fnouns_mod\n",
    "    nénik         mod ;\n",
    "    öntöd         mod ;\n",
    "    pecseny       mod ;\n",
    "\n",
    "LEXICON bnouns_mod\n",
    "    kuty          mod ;\n",
    "    szalm         mod ;\n",
    "    utc           mod ;\n",
    "\n",
    "LEXICON mod\n",
    "    @U.HARM.B@@P.MOD.F@a:@U.HARM.B@@P.MOD.F@a noun_tag ;\n",
    "    @U.HARM.B@@P.MOD.T@a:@U.HARM.B@@P.MOD.T@á noun_tag ;\n",
    "    @U.HARM.F@@P.MOD.F@e:@U.HARM.F@@P.MOD.F@e noun_tag ;\n",
    "    @U.HARM.F@@P.MOD.T@e:@U.HARM.F@@P.MOD.T@é noun_tag ;\n",
    "\n",
    "LEXICON noun_tag \n",
    "    @R.MOD.F@[/N]:@R.MOD.F@0 # ;  ! only non-modified stems (MOD.F) can stop\n",
    "    @D.MOD.F@[/N]:@D.MOD.F@0 plur ;\n",
    "\n",
    "LEXICON ex\n",
    "    @P.EX.T@leges ex  ;\n",
    "                  sup ;\n",
    "\n",
    "LEXICON sup\n",
    "    @P.SUP.T@leg  sup_tag ;\n",
    "    @D.EX@        adjs ;\n",
    "\n",
    "LEXICON sup_tag\n",
    "    [/Supl]:0    adjs ;\n",
    "\n",
    "LEXICON adjs\n",
    "    @U.HARM.F@    fadjs ;\n",
    "    @U.HARM.B@    badjs ;\n",
    "\n",
    "LEXICON fadjs\n",
    "    csendes       adj_tag ;\n",
    "    egészséges    adj_tag ;\n",
    "    idős          adj_tag ;\n",
    "    kék           adj_tag ;\n",
    "    mély          adj_tag ;\n",
    "    öntelt        adj_tag ;\n",
    "    szeles        adj_tag ;\n",
    "    terhes        adj_tag ;\n",
    "    zsémbes       adj_tag ;\n",
    "\n",
    "LEXICON badjs\n",
    "    abszurd       adj_tag ;\n",
    "    bájos         adj_tag ;\n",
    "    finom         adj_tag ;\n",
    "    gyanús        adj_tag ;\n",
    "    okos          adj_tag ;\n",
    "    piros         adj_tag ;\n",
    "    száraz        adj_tag ;\n",
    "    zord          adj_tag ;\n",
    "\n",
    "LEXICON adj_tag\n",
    "    [/Adj]:0       comp ;\n",
    "\n",
    "LEXICON comp \n",
    "    @D.SUP@       plur;\n",
    "    @U.HARM.F@ebb comp_tag ;\n",
    "    @U.HARM.B@abb comp_tag ;\n",
    "\n",
    "LEXICON comp_tag\n",
    "    [_Comp/Adj]:0  plur ;\n",
    "\n",
    "LEXICON plur\n",
    "                                       case ;\n",
    "    @R.VOW@@C.VOW@@C.MOD@k             plur_tag ;  ! only with vowel-ending words, but overrides both vowel-ending (C.VOW) and modified stem (C.MOD)\n",
    "    @D.VOW@@U.HARM.F@ek                plur_tag ;\n",
    "    @D.VOW@@U.HARM.B@ak                plur_tag ;\n",
    "\n",
    "LEXICON plur_tag\n",
    "    [Pl]:0        case ;\n",
    "\n",
    "LEXICON case\n",
    "    @D.MOD.T@            # ;  ! modified stems are not allowed to stop\n",
    "    @R.VOW@@C.VOW@t      case_tag ;\n",
    "    @D.VOW@@U.HARM.F@et  case_tag ;\n",
    "    @D.VOW@@U.HARM.B@at  case_tag ;\n",
    "\n",
    "LEXICON case_tag\n",
    "    [Acc]:0       # ;\n",
    "\"\"\"\n",
    "\n",
    "compile_lexc(grammar, 'nouns.fst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_net('nouns.fst')\n",
    "apply_up('nouns.fst', ['nénikét', 'pecsenyék', 'tűket', 'faodú', 'legeslegeslegszárazabbakat'])        \n",
    "execute_commands('load stack nouns.fst', 'print upper-words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 The allative case\n",
    "\n",
    "Extend the grammar with the allative case. This case differs from accusative in two ways:\n",
    "- it never gets a linking vowel\n",
    "- it has three forms (_-hoz_ / _-hez_ / _-höz_). The first two are used for the words you would expect. The third one is used for words which takes front-vowel inflections, but whose last vowel is a _**ö**_ or _**ő**_ (yes, there is only one of those in the lab word lists).\n",
    "\n",
    "Examples:\n",
    "- _finom_ + `[All]` $\\rightarrow$ _finom**hoz**_\n",
    "- _mély_ + `[All]` $\\rightarrow$ _mély**hez**_\n",
    "- _idős_ + `[All]` $\\rightarrow$ _idős**höz**_\n",
    "\n",
    "Also add the following adjectives to the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = \"\"\"\n",
    "bűnös ! sinful\n",
    "ösztönös ! instinctive\n",
    "felnőtt ! adult\n",
    "dühös ! angry\n",
    "erős ! strong\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "Multichar_Symbols \n",
    "    ! erős[/Adj]höz[All]\t\n",
    "    [/Supl] [/Adj] [_Comp/Adj] [Pl] [Acc]\n",
    "    @P.HARM.F@ @P.HARM.B@ @P.HARM.O@ @D.HARM.B@ @R.HARM.F@ @R.HARM.B@ @R.HARM.O@  ! harmony\n",
    "    @P.EX.T@ @R.EX@ @D.EX@  ! exaggarative\n",
    "    @P.SUP.T@ @R.SUP@ @D.SUP@  ! superlative\n",
    "\n",
    "LEXICON Root\n",
    "                  ex    ;  ! will continue to adjs\n",
    "\n",
    "LEXICON ex\n",
    "    @P.EX.T@leges ex  ;\n",
    "                  sup ;\n",
    "\n",
    "LEXICON sup\n",
    "    @P.SUP.T@leg  sup_tag ;\n",
    "    @D.EX@        adjs ;\n",
    "\n",
    "LEXICON sup_tag\n",
    "    [/Supl]:0    adjs ;\n",
    "\n",
    "LEXICON adjs\n",
    "    @P.HARM.F@    fadjs ;\n",
    "    @P.HARM.B@    badjs ;\n",
    "    @P.HARM.O@    oadjs ;\n",
    "\n",
    "LEXICON fadjs\n",
    "    csendes       adj_tag ;\n",
    "    egészséges    adj_tag ;\n",
    "!    idős          adj_tag ;  ! moved to oeadjs\n",
    "    kék           adj_tag ;\n",
    "    mély          adj_tag ;\n",
    "    öntelt        adj_tag ;\n",
    "    szeles        adj_tag ;\n",
    "    terhes        adj_tag ;\n",
    "    zsémbes       adj_tag ;\n",
    "\n",
    "LEXICON badjs\n",
    "    abszurd       adj_tag ;\n",
    "    bájos         adj_tag ;\n",
    "    finom         adj_tag ;\n",
    "    gyanús        adj_tag ;\n",
    "    okos          adj_tag ;\n",
    "    piros         adj_tag ;\n",
    "    száraz        adj_tag ;\n",
    "    zord          adj_tag ;\n",
    "\n",
    "LEXICON oadjs\n",
    "    idős          adj_tag ;\n",
    "    bűnös         adj_tag ;\n",
    "    ösztönös      adj_tag ;\n",
    "    felnőtt       adj_tag ;\n",
    "    dühös         adj_tag ;\n",
    "    erős          adj_tag ;\n",
    "\n",
    "LEXICON adj_tag\n",
    "    [/Adj]:0       comp ;\n",
    "\n",
    "LEXICON comp \n",
    "    @D.SUP@                 plur;\n",
    "    @D.HARM.B@@P.HARM.F@ebb comp_tag ;\n",
    "    @R.HARM.B@abb           comp_tag ;\n",
    "\n",
    "LEXICON comp_tag\n",
    "    [_Comp/Adj]:0  plur ;\n",
    "\n",
    "LEXICON plur\n",
    "                                case ;\n",
    "    @D.HARM.B@@P.HARM.F@ek      plur_tag ;\n",
    "    @R.HARM.B@ak                plur_tag ;\n",
    "\n",
    "LEXICON plur_tag\n",
    "    [Pl]:0        case ;\n",
    "\n",
    "LEXICON case\n",
    "                   # ;\n",
    "    @R.HARM.F@hez  all_tag ;\n",
    "    @R.HARM.B@hoz  all_tag ;\n",
    "    @R.HARM.O@höz  all_tag ;\n",
    "    @D.HARM.B@et   acc_tag ;\n",
    "    @R.HARM.B@at   acc_tag ;\n",
    "\n",
    "LEXICON acc_tag\n",
    "    [Acc]:0       # ;\n",
    "\n",
    "LEXICON all_tag\n",
    "    [All]:0       # ;\n",
    "\"\"\"\n",
    "\n",
    "compile_lexc(grammar, 'allative.fst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_net('allative.fst')\n",
    "apply_up('allative.fst', ['erősebbekhez', 'erőshöz', 'legeslegeslegpirosabbhoz', 'kéket'])        \n",
    "execute_commands('load stack allative.fst', 'print upper-words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CFG morphology (25 points)\n",
    "\n",
    "### 1.2.1 The basics (15 points)\n",
    "\n",
    "Implement morphological analysis with a CFG grammar. Requirements:\n",
    "- the grammar should handle everything we covered in the lab, except for vowel harmony\n",
    "- there is no need to handle generation; concentrate on parsing\n",
    "- it doesn't matter what the intermediate nonterminals are called. Ideally, _preterminals_ should be valid morphological tags, as in the example below; however, `nlkt.CFG.fromstring()` cannot parse tags such as `[Acc]` as nonterminals. You are free to call them whatever you wish, but descriptive names are encouraged.\n",
    "- encapsulate the functionality in a `CFGMorphParser` class:\n",
    "    - its `__init__()` should accept no parameters (or at least provide defaults)\n",
    "    - it should have a `parse_tree()` method, which accepts a word and returns the parse tree\n",
    "    - it should have a `parse()` method, which accepts a word and returns the morphological parse _in the same form as HFST_. Refer to `hfst_lookup` and the tests for the format. `nltk.tree.Tree.pos()` is a good starting point\n",
    "\n",
    "Note that having the same format as HFST doesn't mean you have to return the exact same output: for instance, we defined _terhes_ as a genuine adjective, even though it is derived from the noun _teher_. So HFST would analyze it as `teher[/N]es[_Adjz:s/Adj][Nom]`, but you only need to return `terhes[/Adj]`. You **also don't have to cover [Nom]**, because of [this bug](https://github.com/nltk/nltk/issues/1890)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('__NOM_1', [Tree('__NOM_2', [Tree('__SUPL_ADJ_2', [Tree('[Supl]', ['leg']), Tree('[/Adj]', ['nagy']), Tree('[_Comp/Adj]', ['obb'])]), Tree('[Pl]', ['ak'])]), Tree('[Acc]', ['at'])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "tree = Tree('__NOM_1', [\n",
    "    Tree('__NOM_2', [\n",
    "        Tree('__SUPL_ADJ_2', [\n",
    "            Tree('[Supl]', ['leg']),\n",
    "            Tree('[/Adj]', ['nagy']),\n",
    "            Tree('[_Comp/Adj]', ['obb']),\n",
    "        ]),\n",
    "        Tree('[Pl]', ['ak']),\n",
    "    ]),\n",
    "    Tree('[Acc]', ['at']),\n",
    "])\n",
    "\n",
    "display(tree)\n",
    "# If display() doesn't work, try this\n",
    "# tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8b2ae5deaaf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# It's OK here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'legfinomabbek'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'terhes'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'terhes[/Adj]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csendesebbet'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'csendes[/Adj]ebb[_Comp/Adj]et[Acc]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'legfinomabbak'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ak[Pl]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your implementation here\n",
    "\n",
    "class CFGMorphParser(object):\n",
    "    def __init__(self):\n",
    "        self.grammar = nltk.CFG.fromstring(\"\"\"\n",
    "        __Nom -> __Nom2 __Acc\n",
    "        __Nom2 -> __SuplAdj __Pl\n",
    "        __SuplAdj -> __Supl __Adj __Comp | __Adj __Comp | __Adj\n",
    "        __Supl -> __Ex __Sup | __Sup\n",
    "        __Ex -> 'l' 'e' 'g' 'e' 's'\n",
    "        __Sup -> 'l' 'e' 'g'\n",
    "        __Adj -> 't' 'e' 'r' 'h' 'e' 's' | 'c' 's' 'e' 'n' 'd' 'e' 's' | 'f' 'i' 'n' 'o' 'm'\n",
    "        __Comp -> 'a' 'b' 'b' | 'e' 'b' 'b'\n",
    "        __Pl -> 'a' 'k' | 'e' 'k'\n",
    "        __Acc -> 'a' 't' | 'e' 't'\n",
    "        \"\"\")\n",
    "        self.p = nltk.ChartParser(self.grammar)\n",
    "        \n",
    "    def parse(self, word):\n",
    "        try:\n",
    "            trees = self.p.parse([char for char in word])\n",
    "        except ValueError:\n",
    "            return None\n",
    "        for tree in trees:\n",
    "            print(tree)\n",
    "            display(tree)\n",
    "            return ''.join(tree.leaves())\n",
    "    \n",
    "    \n",
    "    def parse_tree(self, word):\n",
    "        return Tree(word, [word])\n",
    "\n",
    "    # nltk.tree.Tree.pos\n",
    "    def pos(self):\n",
    "        pos = []\n",
    "        for child in self:\n",
    "            if isinstance(child, Tree):\n",
    "                pos.extend(child.pos())\n",
    "            else:\n",
    "                pos.append((child, self._label))\n",
    "        return pos\n",
    "\n",
    "    \n",
    "# Tests\n",
    "parser = CFGMorphParser()\n",
    "\n",
    "print(parser.parse('unknown_word'))\n",
    "print(parser.parse('terhes'))\n",
    "print(parser.parse('csendesebbet'))\n",
    "print(parser.parse('legfinomabbak'))\n",
    "print(parser.parse('legfinomabbek'))\n",
    "\n",
    "assert parser.parse('unknown_word') is None\n",
    "assert parser.parse('terhes') == 'terhes[/Adj]'\n",
    "assert parser.parse('csendesebbet') == 'csendes[/Adj]ebb[_Comp/Adj]et[Acc]'\n",
    "assert parser.parse('legfinomabbak') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ak[Pl]'\n",
    "# It's OK here\n",
    "assert parser.parse('legfinomabbek') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ek[Pl]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Vowel harmony (10 points)\n",
    "\n",
    "Also handle vowel harmony. Write a function that traverses the tree manually (similarly to [exercise 2.4 in the lab](../../course_material/10_Syntax/10_Syntax_lab_solutions.ipynb#2.4-Evaluation*)) and returns `True` or `False`, depending on whether the tree conforms to vowel harmony rules. Use this function in `parse_tree` (and `parse`) to filter invalid trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert parser.parse('legfinomabbak') == 'leg[/Supl]finom[/Adj]abb[_Comp/Adj]ak[Pl]'\n",
    "assert parser.parse('legfinomabbek') == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Syntax (55 points)\n",
    "\n",
    "In this exercise, you will parse a treebank, and induce a PCFG grammar from it. You will then implement a probabilistic version of the CKY algorithm, and evaluate the grammar on the test split of the treebank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Parse a treebank (10 points)\n",
    "\n",
    "Parse the treebank file `en_lines-ud-train.s` in the notebook's directory. Write a **generator** function that reads the file and yields `nltk.tree.Tree` objects. In particular,\n",
    "- do not read the whole file into memory\n",
    "- the `Tree.fromstring()` function converts an s-expression into a tree\n",
    "\n",
    "Open the file in an editor to see the formatting.\n",
    "\n",
    "Note that the file was created by parsing the [LinES dependency corpus](https://github.com/UniversalDependencies/UD_English-LinES/tree/master) with [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/), so it is not a gold standard by any means, but it will suffice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "\n",
    "def parse_treebank(treebank_file):\n",
    "    with open(treebank_file, 'r') as f:\n",
    "        s = ''\n",
    "        for l in f:\n",
    "            if len(l) > 1:\n",
    "                s += l\n",
    "            elif s:\n",
    "                try:\n",
    "                    t = Tree.fromstring(s)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                finally:\n",
    "                    s = ''\n",
    "                yield t\n",
    "            else:\n",
    "                s = ''\n",
    "            \n",
    "# Tests\n",
    "assert sum(1 for _ in parse_treebank('en_lines-ud-train.s')) == 2613\n",
    "assert isinstance(next(parse_treebank('en_lines-ud-train.s')), Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filter trees (5 points)\n",
    "\n",
    "In order to avoid problems further down the line, we shall only handle a subset of the trees in the treebank. We call a tree _valid_, if\n",
    "- its root is `'S'`\n",
    "- the root has at least two children.\n",
    "\n",
    "Write a function that returns `True` for \"valid\" trees and `False` for invalid ones. Filter the your generator with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tree_valid(tree):\n",
    "    if not isinstance(tree, Tree):\n",
    "        return False\n",
    "    if tree.label() != 'S':\n",
    "        return False\n",
    "    if len(tree) < 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Tests\n",
    "assert sum(map(is_tree_valid, parse_treebank('en_lines-ud-train.s'))) == 2311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Induce the PCFG grammar (10 points)\n",
    "\n",
    "Now that you have the trees, it is time to induce (train) a PCFG grammar for it! Luckily, `nltk` has a functions for just that: [`nltk.grammar.induce_pcfg`](http://www.nltk.org/api/nltk.html#nltk.grammar.induce_pcfg). Use it to acquire your PCFG grammar. You can find hints at how to use it in the [grammar module](http://www.nltk.org/_modules/nltk/grammar.html).\n",
    "\n",
    "Note: since we want to parse sentences with the PCKY algorithm, we need our grammar to be in CNF. Unfortunately, `nlkt` cannot convert a grammar to CNF, so you have to ensure that the trees are in CNF before feeding them to the PCFG induction function. That way, we can be sure that our grammar will be also. There are two functions that ensure a tree is in CNF:\n",
    "- [`collapse_unary`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.collapse_unary). Make sure you call it with `collapsePOS=True`!\n",
    "- [`chomsky_normal_form`](http://www.nltk.org/api/nltk.html#nltk.tree.Tree.chomsky_normal_form). Do not use any smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grammar(trees):\n",
    "    prods = []\n",
    "    for tree in trees: \n",
    "        tree.collapse_unary(collapsePOS=True)\n",
    "        tree.chomsky_normal_form()\n",
    "        prods += tree.productions()\n",
    "    return nltk.grammar.induce_pcfg(nltk.Nonterminal('S'), prods)\n",
    "        \n",
    "def is_grammar_cnf(grammar):\n",
    "    for prod in grammar.productions():\n",
    "        rhs = prod.rhs()\n",
    "        if len(rhs) > 2 or (len(rhs) == 1 and isinstance(rhs[0], nltk.Nonterminal)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Tests\n",
    "grammar = train_grammar(filter(is_tree_valid, parse_treebank('en_lines-ud-train.s')))\n",
    "assert is_grammar_cnf(grammar)\n",
    "assert len(grammar.productions()) == 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Implement PCKY (15 points)\n",
    "\n",
    "Implement the PCKY algorithm. Encapsulate it in a class called `PCKYParser`. Extend your `CKYParser` solution from the lab so that it creates trees with probabilities (`ProbabilisticTree`). The `parse()` method should also accept a parameter `n`, and only return the most probable `n` trees (as a generator).\n",
    "\n",
    "Some pointers:\n",
    "- [ProbabilisticTree](http://www.nltk.org/api/nltk.html#nltk.tree.ProbabilisticTree), which inherits from\n",
    "- [ProbabilisticMixIn](http://www.nltk.org/api/nltk.html#nltk.probability.ProbabilisticMixIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tree import ProbabilisticTree as PTree\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class PCKYParser(object):\n",
    "    def __init__(self, grammar):\n",
    "        if not isinstance(grammar, nltk.CFG):\n",
    "            raise TypeError(\"g\")\n",
    "        self.grammar = grammar\n",
    "    \n",
    "    def parse(self, sent, n=1):\n",
    "        k = len(sent)\n",
    "        # init\n",
    "        cky = np.empty((k,k), dtype=object)\n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                cky[i, j] = []\n",
    "        # lexical rules\n",
    "        for i, word in enumerate(sent):\n",
    "            for prod in self.grammar.productions():\n",
    "                if word in prod.rhs():\n",
    "                    cky[i, i].append(PTree(prod.lhs(), [word], prob=prod.prob()))\n",
    "        # production rules\n",
    "        for col in range(1, k):\n",
    "            for row in range(col-1, -1, -1):\n",
    "                ways_to_split = col-row  # \"distance\" from diag\n",
    "                for w in range(ways_to_split):\n",
    "                    left = ways_to_split - w\n",
    "                    down = 1 + w\n",
    "                    for prod in self.grammar.productions():\n",
    "                        if len(prod.rhs()) < 2:\n",
    "                            continue  # skip lexical productions\n",
    "                        # because of CNF, all non-lexical rules have 2 constituents on rhs\n",
    "                        ls = prod.rhs()[0]  # left symbol\n",
    "                        ds = prod.rhs()[1]  # down symbol\n",
    "                        for lt in cky[row, col-left]:  # left tree\n",
    "                            for dt in cky[row+down, col]:  # right tree\n",
    "                                if lt.label() == ls and dt.label() == ds:\n",
    "                                    # multiply tree and production probabilities\n",
    "                                    p = lt.prob() * dt.prob() * prod.prob()\n",
    "                                    cky[row, col].append(PTree(prod.lhs(), [lt, dt], prob=p))\n",
    "        # collect sentences from parsed trees, sort them by probability\n",
    "        sents = sorted(\n",
    "            [tree for tree in cky[0, k-1] if tree.label() == nltk.Nonterminal('S')],\n",
    "            key=lambda tree: tree.prob()\n",
    "        )\n",
    "        # yield the most probable n trees\n",
    "        for sent in deque(sents, maxlen=n):\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = train_grammar(filter(is_tree_valid, parse_treebank('en_lines-ud-train.s')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all trees:\n",
      "(S (NP (NN A) (NN user)) (VP+VBZ writes)) (p=1.36211e-13)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "ProbabilisticTree(S, [ProbabilisticTree(NP, [ProbabilisticTree(NN, ['A']) (p=0.0005569996286669142), ProbabilisticTree(NN, ['user']) (p=0.0014853323431117712)]) (p=4.914493515617823e-09), ProbabilisticTree(VP+VBZ, ['writes']) (p=0.03125)]) (p=1.3621101761690197e-13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DT A) (NN user)) (VP+VBZ writes)) (p=8.74454e-11)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "ProbabilisticTree(S, [ProbabilisticTree(NP, [ProbabilisticTree(DT, ['A']) (p=0.010554682236694363), ProbabilisticTree(NN, ['user']) (p=0.0014853323431117712)]) (p=3.155030665039823e-06), ProbabilisticTree(VP+VBZ, ['writes']) (p=0.03125)]) (p=8.744541754544963e-11)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "most probable tree:\n",
      "(S (NP (DT A) (NN user)) (VP+VBZ writes)) (p=8.74454e-11)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "ProbabilisticTree(S, [ProbabilisticTree(NP, [ProbabilisticTree(DT, ['A']) (p=0.010554682236694363), ProbabilisticTree(NN, ['user']) (p=0.0014853323431117712)]) (p=3.155030665039823e-06), ProbabilisticTree(VP+VBZ, ['writes']) (p=0.03125)]) (p=8.744541754544963e-11)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tests\n",
    "pparser = PCKYParser(grammar)\n",
    "trees = pparser.parse('A user writes'.split(), n=3)\n",
    "print('all trees:')\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    display(tree)\n",
    "print('-'*80)\n",
    "# let's see if it returns the most probable tree when n=1\n",
    "trees = pparser.parse('A user writes'.split(), n=1)\n",
    "print('most probable tree:')\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Evaluate the grammar (15 points)\n",
    "\n",
    "Evaluate your grammar on the test split of the treebank (`en_lines-ud-dev.s`). Implement the **unlabelled** PARSEVAL metric. See [the first answer for an example](https://linguistics.stackexchange.com/questions/1873/is-there-a-well-established-metric-to-measure-the-effectiveness-of-a-parsing-alg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituents(tree):\n",
    "    consts = []\n",
    "    if isinstance(tree, Tree) and isinstance(tree[0], Tree):\n",
    "        consts.append(' '.join(tree.leaves()))\n",
    "        for subtree in tree:\n",
    "            c = constituents(subtree)\n",
    "            if type(c) is str:\n",
    "                consts.append(c)\n",
    "            else:\n",
    "                consts += c\n",
    "    else:\n",
    "        consts = tree[0]\n",
    "    return consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A user writes', 'A user', 'A', 'user', 'writes']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for tree in pparser.parse('A user writes'.split(), n=1):\n",
    "    print(constituents(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseval_tree(gold, cand):\n",
    "    gold_consts = constituents(gold)\n",
    "    cand_consts = constituents(cand)\n",
    "    correct_cand = [c for c in cand_consts if c in gold_consts]\n",
    "    correct_gold = [c for c in gold_consts if c in cand_consts]\n",
    "    precision = len(correct_cand) / len(cand_consts)\n",
    "    recall = len(correct_gold) / len(gold_consts)\n",
    "    F = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, F\n",
    "    \n",
    "\n",
    "def parseval(gold_trees, parser):\n",
    "    precisions, recalls, Fs = [], [], []\n",
    "    # get values for each sentence\n",
    "    for gold_tree in gold_trees:\n",
    "        sentence = gold_tree.leaves()\n",
    "        if len(sentence) > 10: continue\n",
    "        print('evaluating sentece:\\n\\t', ' '.join(sentence))\n",
    "        try:\n",
    "            cand_tree = next(parser.parse(sentence, n=1))  # TODO better solution?\n",
    "            p, r, F = parseval_tree(gold_tree, cand_tree)\n",
    "        except StopIteration:\n",
    "            p, r, F = 0.0, 0.0, 0.0\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        Fs.append(F)\n",
    "    # return averages\n",
    "    return [sum(v) / len(v) for v in [precisions, recalls, Fs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pparser = PCKYParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating sentece:\n",
      "\t Access supports the XML Schema standard -LRB- XSD -RRB- .\n",
      "evaluating sentece:\n",
      "\t Combining multiple filters Filters are additive .\n",
      "evaluating sentece:\n",
      "\t Conditional filters are independent of each other .\n",
      "evaluating sentece:\n",
      "\t Pay any price .\n",
      "evaluating sentece:\n",
      "\t Thank you for your help .\n",
      "evaluating sentece:\n",
      "\t It was now past five o'clock .\n",
      "evaluating sentece:\n",
      "\t Quinn had trouble finding a seat .\n",
      "evaluating sentece:\n",
      "\t After that , a strange thing happened .\n",
      "evaluating sentece:\n",
      "\t Quinn guessed her age at around twenty .\n",
      "evaluating sentece:\n",
      "\t Quinn smiled weakly .\n",
      "evaluating sentece:\n",
      "\t No problem , he said .\n",
      "evaluating sentece:\n",
      "\t I was just wondering if you liked the book .\n",
      "evaluating sentece:\n",
      "\t The girl shrugged .\n",
      "evaluating sentece:\n",
      "\t I 've read better and I 've read worse .\n",
      "evaluating sentece:\n",
      "\t The girl shrugged again and cracked her gum loudly .\n",
      "evaluating sentece:\n",
      "\t Yeah , he 's smart .\n",
      "evaluating sentece:\n",
      "\t But he talks too much .\n",
      "evaluating sentece:\n",
      "\t You 'd like more action ?\n",
      "evaluating sentece:\n",
      "\t I guess so .\n",
      "evaluating sentece:\n",
      "\t I do n't know .\n",
      "evaluating sentece:\n",
      "\t The girl shrugged once again .\n",
      "evaluating sentece:\n",
      "\t It passes the time , I guess .\n",
      "evaluating sentece:\n",
      "\t Anyway , it 's no big deal .\n",
      "evaluating sentece:\n",
      "\t It 's just a book .\n",
      "evaluating sentece:\n",
      "\t The girl was beyond hope .\n",
      "evaluating sentece:\n",
      "\t He told himself to stay calm .\n",
      "evaluating sentece:\n",
      "\t But that did little good .\n",
      "evaluating sentece:\n",
      "\t Soon the people were surging around him .\n",
      "evaluating sentece:\n",
      "\t He watched .\n",
      "evaluating sentece:\n",
      "\t I wish it was .\n",
      "evaluating sentece:\n",
      "\t But this has nothing to do with literature .\n",
      "evaluating sentece:\n",
      "\t Quinn realized that he was talking nonsense .\n",
      "evaluating sentece:\n",
      "\t The private detective , he repeated softly .\n",
      "evaluating sentece:\n",
      "\t You 're the only one in the book .\n",
      "evaluating sentece:\n",
      "\t But I 'm not a detective .\n",
      "evaluating sentece:\n",
      "\t I 'm a writer .\n",
      "evaluating sentece:\n",
      "\t I 'm sorry .\n",
      "evaluating sentece:\n",
      "\t Auster said .\n",
      "evaluating sentece:\n",
      "\t But that 's what I happen to be .\n",
      "evaluating sentece:\n",
      "\t The whole thing is a bad dream .\n",
      "evaluating sentece:\n",
      "\t I have no idea what you 're talking about .\n",
      "evaluating sentece:\n",
      "\t Quinn told him .\n",
      "evaluating sentece:\n",
      "\t I even have proof .\n",
      "evaluating sentece:\n",
      "\t He handed it to Auster .\n",
      "evaluating sentece:\n",
      "\t You see , he said .\n",
      "evaluating sentece:\n",
      "\t It 's even made out to you .\n",
      "evaluating sentece:\n",
      "\t Auster looked the check over carefully and nodded .\n",
      "evaluating sentece:\n",
      "\t It seems to be a perfectly normal check .\n",
      "evaluating sentece:\n",
      "\t I want you to have it .\n",
      "evaluating sentece:\n",
      "\t I could n't possibly accept it .\n",
      "evaluating sentece:\n",
      "\t It 's of no use to me .\n",
      "evaluating sentece:\n",
      "\t Quinn looked around the apartment and gestured vaguely .\n",
      "evaluating sentece:\n",
      "\t Buy yourself some more books .\n",
      "evaluating sentece:\n",
      "\t This is money you 've earned .\n",
      "evaluating sentece:\n",
      "\t You deserve to have it yourself .\n",
      "evaluating sentece:\n",
      "\t Auster paused for a moment .\n",
      "evaluating sentece:\n",
      "\t Quinn did not say anything .\n",
      "evaluating sentece:\n",
      "\t Auster asked .\n",
      "evaluating sentece:\n",
      "\t All right , said Quinn at last .\n",
      "evaluating sentece:\n",
      "\t We 'll see what happens .\n",
      "evaluating sentece:\n",
      "\t I do n't understand it at all .\n",
      "evaluating sentece:\n",
      "\t Wires sometimes get crossed .\n",
      "evaluating sentece:\n",
      "\t Yes , that 's happened to me before .\n",
      "evaluating sentece:\n",
      "\t I 've never heard of the Stillmans .\n",
      "evaluating sentece:\n",
      "\t I do n't hang around with people like that .\n",
      "evaluating sentece:\n",
      "\t You never know .\n",
      "evaluating sentece:\n",
      "\t It 's a real case with real people .\n",
      "evaluating sentece:\n",
      "\t I 'm aware of that .\n",
      "evaluating sentece:\n",
      "\t Quinn realized that he should be going .\n",
      "evaluating sentece:\n",
      "\t Nevertheless , he was reluctant to move .\n",
      "evaluating sentece:\n",
      "\t His church is venerable rich and beautiful .\n",
      "evaluating sentece:\n",
      "\t The antique tiles are gorgeous .\n",
      "evaluating sentece:\n",
      "\t But all these things are in some way external .\n",
      "evaluating sentece:\n",
      "\t We outsiders are not stable enough to appreciate them .\n",
      "evaluating sentece:\n",
      "\t We soon get around to contemporary matters .\n",
      "evaluating sentece:\n",
      "\t He is a genius .\n",
      "evaluating sentece:\n",
      "\t The Russians are in disarray , perhaps in retreat .\n",
      "evaluating sentece:\n",
      "\t Such intelligent discussion has n't always been wrong .\n",
      "evaluating sentece:\n",
      "\t There are raisons d'etat and there are private crimes .\n",
      "evaluating sentece:\n",
      "\t And the talk goes on .\n",
      "evaluating sentece:\n",
      "\t Tatu agrees with the Archbishop about the Russians .\n",
      "evaluating sentece:\n",
      "\t Dessert is served .\n",
      "evaluating sentece:\n",
      "\t The intellectual leaders of the Enlightenment were decidedly anti-Semitic .\n",
      "evaluating sentece:\n",
      "\t I made sure that my letter would be delivered .\n",
      "evaluating sentece:\n",
      "\t The letter was never acknowledged .\n",
      "evaluating sentece:\n",
      "\t It supports terrorists .\n",
      "evaluating sentece:\n",
      "\t It is friendlier to Amin than to Rabin .\n",
      "evaluating sentece:\n",
      "\t Arab boys are racing their donkeys down the hill .\n",
      "evaluating sentece:\n",
      "\t I shrug .\n",
      "evaluating sentece:\n",
      "\t As soon as they leave they are forgotten .\n",
      "evaluating sentece:\n",
      "\t We 'd only met ten minutes before .\n",
      "evaluating sentece:\n",
      "\t He took me to his good bosom .\n",
      "evaluating sentece:\n",
      "\t His eyes began to mist .\n",
      "evaluating sentece:\n",
      "\t His heart opened to me .\n",
      "evaluating sentece:\n",
      "\t It opened like a cuckoo clock .\n",
      "evaluating sentece:\n",
      "\t the librarian says .\n",
      "evaluating sentece:\n",
      "\t she asked .\n",
      "evaluating sentece:\n",
      "\t `` Difficult to guess , '' I tell her .\n",
      "evaluating sentece:\n",
      "\t Dostoevski asks .\n",
      "evaluating sentece:\n",
      "\t Perhaps there is a certain Vautrin-admiring romanticism in this .\n",
      "evaluating sentece:\n",
      "\t The old one sat on her chair .\n",
      "evaluating sentece:\n",
      "\t She glanced at me above the glasses .\n",
      "evaluating sentece:\n",
      "\t An eerie feeling came over me .\n",
      "evaluating sentece:\n",
      "\t She seemed uncanny and fateful .\n",
      "evaluating sentece:\n",
      "\t There was yet a visit to the doctor .\n",
      "evaluating sentece:\n",
      "\t He became very cool and collected all at once .\n",
      "evaluating sentece:\n",
      "\t I asked .\n",
      "evaluating sentece:\n",
      "\t He smiled , as if at some quiet joke .\n",
      "evaluating sentece:\n",
      "\t So you are going out there .\n",
      "evaluating sentece:\n",
      "\t he asked , in a matter-of-fact tone .\n",
      "evaluating sentece:\n",
      "\t I felt very annoyed .\n",
      "evaluating sentece:\n",
      "\t Is that question in the interests of science too ?\n",
      "evaluating sentece:\n",
      "\t I interrupted .\n",
      "evaluating sentece:\n",
      "\t The mere wealth I leave to others .\n",
      "evaluating sentece:\n",
      "\t Avoid irritation more than exposure to the sun .\n",
      "evaluating sentece:\n",
      "\t I found her triumphant .\n",
      "evaluating sentece:\n",
      "\t The work was going on .\n",
      "evaluating sentece:\n",
      "\t They were dying slowly -- it was very clear .\n",
      "evaluating sentece:\n",
      "\t Moreover , I respected the fellow .\n",
      "evaluating sentece:\n",
      "\t That 's backbone .\n",
      "evaluating sentece:\n",
      "\t It was difficult .\n",
      "evaluating sentece:\n",
      "\t She had a distaste for the work .\n",
      "evaluating sentece:\n",
      "\t This man had verily accomplished something .\n",
      "evaluating sentece:\n",
      "\t Sometimes he stood up for exercise .\n",
      "evaluating sentece:\n",
      "\t He began to write again .\n",
      "evaluating sentece:\n",
      "\t The sick man was too ill to groan .\n",
      "evaluating sentece:\n",
      "\t The flies buzzed in a great peace .\n",
      "evaluating sentece:\n",
      "\t Fourthly , do not say we were not available !\n",
      "evaluating sentece:\n",
      "\t A strong euro will therefore penalise European agricultural exports .\n",
      "evaluating sentece:\n",
      "\t Too little is being done in this area .\n",
      "evaluating sentece:\n",
      "\t Thank you , Mr Donnay .\n",
      "evaluating sentece:\n",
      "\t Our group has tabled several amendments in this direction .\n",
      "evaluating sentece:\n",
      "\t The decisions were adopted unanimously .\n",
      "evaluating sentece:\n",
      "\t The joint debate is closed .\n",
      "evaluating sentece:\n",
      "\t That concludes Parliament 's agenda .\n",
      "evaluating sentece:\n",
      "\t That is wrong .\n",
      "evaluating sentece:\n",
      "\t This indexing also includes the notion of an accelerator .\n",
      "evaluating sentece:\n",
      "\t Everyone will therefore understand why I am rejecting them .\n",
      "evaluating sentece:\n",
      "\t Such a situation is unacceptable .\n",
      "evaluating sentece:\n",
      "\t We must help them to improve their border security .\n",
      "evaluating sentece:\n",
      "\t The debate is closed .\n",
      "evaluating sentece:\n",
      "\t Chemicals are still on our agenda in the EU .\n",
      "evaluating sentece:\n",
      "\t But that is not really my problem .\n",
      "evaluating sentece:\n",
      "\t What I criticise is the spirit underlying these amendments .\n",
      "evaluating sentece:\n",
      "\t The second issue is that of incentives .\n",
      "evaluating sentece:\n",
      "\t The Member States would have to do so .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating sentece:\n",
      "\t At six Roland Dando came home .\n",
      "evaluating sentece:\n",
      "\t Struggle my arse .\n",
      "evaluating sentece:\n",
      "\t Roly glared .\n",
      "evaluating sentece:\n",
      "\t Nobody gives a damn where he is .\n",
      "evaluating sentece:\n",
      "\t But he is in town , now ?\n",
      "evaluating sentece:\n",
      "\t That 's not possible .\n",
      "evaluating sentece:\n",
      "\t He 's not come up to town ?\n",
      "evaluating sentece:\n",
      "\t But that 's ridiculous , Roly .\n",
      "evaluating sentece:\n",
      "\t You know Shinza .\n",
      "evaluating sentece:\n",
      "\t He knows what he wants .\n",
      "evaluating sentece:\n",
      "\t Of course he should have got Foreign Affairs .\n",
      "evaluating sentece:\n",
      "\t But that 's between the two of them .\n",
      "evaluating sentece:\n",
      "\t He was given to putting himself on strange mixtures .\n",
      "evaluating sentece:\n",
      "\t Oh Mweta 's not like that .\n",
      "evaluating sentece:\n",
      "\t You know Mweta .\n",
      "evaluating sentece:\n",
      "\t I know Mweta .\n",
      "evaluating sentece:\n",
      "\t But there 's the President , now .\n",
      "evaluating sentece:\n",
      "\t Dando did not explain the shift of reference .\n",
      "evaluating sentece:\n",
      "\t Dando was gleeful .\n",
      "evaluating sentece:\n",
      "\t They 're just not going to find any .\n",
      "evaluating sentece:\n",
      "\t She wore a dress made of Congo cloth .\n",
      "evaluating sentece:\n",
      "\t Same as yours , I believe .\n",
      "evaluating sentece:\n",
      "\t But they call me James .\n",
      "evaluating sentece:\n",
      "\t I should damn well hope so .\n",
      "evaluating sentece:\n",
      "\t We could just sweep the others off the floor .\n",
      "evaluating sentece:\n",
      "\t Get her to sing , Dando called out proudly .\n",
      "evaluating sentece:\n",
      "\t she asked Bray .\n",
      "evaluating sentece:\n",
      "\t Well , what 'd you think ?\n",
      "evaluating sentece:\n",
      "\t I 've got a voice like a bullfrog .\n",
      "evaluating sentece:\n",
      "\t Yes , lovely creature , is n't she ?\n",
      "evaluating sentece:\n",
      "\t You did n't leave her with Ras ?\n",
      "evaluating sentece:\n",
      "\t He moved his shoulders helplessly .\n",
      "evaluating sentece:\n",
      "\t Children swept in and out , belligerently pleasure-seeking .\n",
      "evaluating sentece:\n",
      "\t Babies slept in dark rooms .\n",
      "evaluating sentece:\n",
      "\t Food was cooked by many hands .\n",
      "evaluating sentece:\n",
      "\t Dobby blinked anxiously up at Harry .\n",
      "evaluating sentece:\n",
      "\t Harry was n't listening .\n",
      "evaluating sentece:\n",
      "\t With a crack like a whip , Dobby vanished .\n",
      "evaluating sentece:\n",
      "\t Harry took it .\n",
      "evaluating sentece:\n",
      "\t It did not contain birthday greetings .\n",
      "evaluating sentece:\n",
      "\t Enjoy your holidays !\n",
      "evaluating sentece:\n",
      "\t Uncle Vernon was as bad as his word .\n",
      "evaluating sentece:\n",
      "\t The room was growing dark .\n",
      "evaluating sentece:\n",
      "\t Mrs Weasley beamed down at him .\n",
      "evaluating sentece:\n",
      "\t This is a gnome , he said grimly .\n",
      "evaluating sentece:\n",
      "\t It was certainly nothing like Father Christmas .\n",
      "evaluating sentece:\n",
      "\t Just then , the front door slammed .\n",
      "evaluating sentece:\n",
      "\t He 's back !\n",
      "evaluating sentece:\n",
      "\t said George .\n",
      "evaluating sentece:\n",
      "\t said Fred eagerly .\n",
      "evaluating sentece:\n",
      "\t Mr Weasley 's eyes jerked open .\n",
      "evaluating sentece:\n",
      "\t He stared guiltily at his wife .\n",
      "evaluating sentece:\n",
      "\t Mr Weasley blinked .\n",
      "evaluating sentece:\n",
      "\t He looked around , saw Harry , and jumped .\n",
      "evaluating sentece:\n",
      "\t On the third landing , a door stood ajar .\n",
      "evaluating sentece:\n",
      "\t Not like that room you had with the Muggles .\n",
      "evaluating sentece:\n",
      "\t Ron 's ears went pink .\n",
      "------------------------------------------------------------------------------------------\n",
      "average precision: 0.39529280810679\n",
      "average recall: 0.4639519728562668\n",
      "average F-score: 0.426364555443786\n"
     ]
    }
   ],
   "source": [
    "gold_trees = filter(is_tree_valid, parse_treebank('en_lines-ud-dev.s'))\n",
    "\n",
    "ap, ar, aF = parseval(gold_trees, pparser)\n",
    "\n",
    "print('-'*90)\n",
    "print('average precision:', ap)\n",
    "print('average recall:', ar)\n",
    "print('average F-score:', aF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Bonus* (20 points)\n",
    "\n",
    "Implement a class that converts Python-style regular expressions to XSLT-style ones, and executes them via foma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Conversion* (10 points)\n",
    "\n",
    "The functionality should be encapsulated in a class called `FomaRegex`. The public API specification is as follows:\n",
    "- its constructor should accept a valid Python regex string (not a [regex object](https://docs.python.org/3/library/re.html#regular-expression-objects)), convert it to the XFST format and store it in its `pattern` member field\n",
    "- it should have a `convert` static method that does the pattern conversion. You can use pure Python or better yet, a CFG grammar\n",
    "- the class should implement the context manager protocol:\n",
    "    - when entering the context, an FSA file should be created via foma and its name stored in the `fsa_file` field.  The [`regex <regex> ;` command](https://github.com/mhulden/foma/blob/master/foma/docs/simpleintro.md#compiling-regular-expressions) can be used to compile a regex in foma; for the rest, refer to the [`compile_lexc()` function](../../course_material/09_Morphology_lab/09_Morphology_lab_solutions.ipynb#Morphology)\n",
    "    - after the context closes, the FSA file should be deleted and the `fsa_file` member set to `None`\n",
    "\n",
    "You only need to account for the first six rows in [the table comparing the two syntaxes](../../course_material/08_Morphology/08_Morphology_lecture.ipynb#XFST-vs-Python-regular-expressions). Additionally, you only need to cover the characters a-zA-Z0-9 (i.e. no punctuation). Note that there are two options for verbatim texts in XFST: `[a b c]` or `{abc}`. You are encouraged to use the latter; should you choose to use the former, update the assert statements accordingly.\n",
    "\n",
    "You don't have to worry about applying the regex at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class FomaRegex:\n",
    "    pass\n",
    "\n",
    "# Tests\n",
    "assert FomaRegex.convert('ab?c*d+') == '{a}{b}^<2{c}*{d}+'\n",
    "assert FomaRegex.convert('a.b') == '{a}?{b}'\n",
    "assert FomaRegex.convert('a+(bc|de).*') == '{a}+[{bc}|{de}]?*'\n",
    "\n",
    "with FomaRegex('a.b') as fr:\n",
    "    assert fr.pattern == '{a}?{b}', 'Invalid pattern'\n",
    "    assert fr.fsa_file is not None, 'FSA file is None in with'\n",
    "    fsa_file = fr.fsa_file\n",
    "assert fr.fsa_file is None, 'FSA file is not None after with'\n",
    "assert not os.path.isfile(fsa_file), 'FSA file still exists after with'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Application* (5 points)\n",
    "\n",
    "Add a `match` method to the class that runs the regex against the specified string. It should return `True` or `False` depending on whether the regex matched the string.\n",
    "\n",
    "Note: obviously you should use your FSA file and foma, not the `re` module. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "with FomaRegex('a*(bc|de).+') as fr:\n",
    "    assert fr.match('aabcd') is True\n",
    "    assert fr.match('ade') is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multiple regexes (5 points)\n",
    "\n",
    "Make sure not all `FomaRegex` objects use the same FSA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "with FomaRegex('a') as a, FomaRegex('b') as b:\n",
    "    assert a.fsa_file != b.fsa_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
