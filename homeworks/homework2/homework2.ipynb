{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Katona Máté (PD6YOR)\n",
    "\n",
    "\n",
    "# Homework 2\n",
    "\n",
    "The maximum score of this homework is 100 points. Grading is listed in this table:\n",
    "\n",
    "| Grade | Score range |\n",
    "| --- | --- |\n",
    "| 5 | 85+ |\n",
    "| 4 | 70-84 |\n",
    "| 3 | 55-69 |\n",
    "| 2 | 40-54 |\n",
    "| 1 | 0-39 |\n",
    "\n",
    "Most exercises include tests which should pass if your solution is correct.\n",
    "However successful test do not guarantee that your solution is correct.\n",
    "You are free to add more tests.\n",
    "\n",
    "## Deadline\n",
    "__2017 November 20<sup>th</sup> Monday 23:59__\n",
    "\n",
    "\n",
    "# Main exercise (60 points)\n",
    "\n",
    "Implement the Viterbi algorithm fot $k=3$.\n",
    "\n",
    "The input can be found [here](http://sandbox.hlt.bme.hu/~gaebor/ea_anyag/python_nlp/).\n",
    "You can use the 1, 10 or 100 million word corpus, it is advised to use the 1 million corpus while you are developing and you can try te larger ones afterwards.\n",
    "\n",
    "Write a class called `Viterbi` with the following attributes:\n",
    "* `__init__`: has no arguments, except `self`\n",
    "* `train`: one argument (aside `self`), an iterable object which generates 2-tuples of strings `[(word1, pos1), (word2, pos2), ...]`\n",
    "  * initializes the vocabularies, empirical probabilities or any other data attributes needed for the algorithm.\n",
    "  * you can read the data (generator) only once\n",
    "  * returns `None`\n",
    "* predict: one argument (aside `self`), an iterable object, a list of words.\n",
    "  * returns the predicted label sequence: a list of labels with the same length as the input.\n",
    "\n",
    "Don't use global variables!\n",
    "\n",
    "### Hint\n",
    "* Use a 3-dimensional array for the transition probabilities $\\mathbb{P}(v \\ | \\ w, u)$.\n",
    "* Use a 3-dimensional array for the Viterbi table, one index for time, one index for the previous state and one index for the state before that.\n",
    "* Same for the backtracking table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotTrainedException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Viterbi(object):\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "        \n",
    "    def train(self, data):\n",
    "        # copy and count data - ugly\n",
    "        data = [(w, t) for w, t in data]\n",
    "        dowi, doti = {}, {}\n",
    "        for w, t in data:\n",
    "            dowi[w] = dowi.get(w, len(dowi))\n",
    "            doti[t] = doti.get(t, len(doti))\n",
    "        self.doiw = {i: w for w, i in dowi.items()}\n",
    "        self.doit = {i: t for t, i in doti.items()}\n",
    "        self.dowi = dowi\n",
    "        self.doti = doti\n",
    "        self.W = len(dowi)  # number of words\n",
    "        self.T = len(doti)  # number of tags\n",
    "        # probs\n",
    "        word_tags = np.zeros((self.W, self.T), dtype=np.int32)  # number of (word, tag) occurences\n",
    "        tag_seqs = np.zeros((self.T, self.T, self.T), dtype=np.int32)  # number of (tag, tag, tag) occurences\n",
    "        prev_ti = None\n",
    "        prev_prev_ti = None\n",
    "        for w, t in data:\n",
    "            ti = doti[t]\n",
    "            word_tags[dowi[w], ti] += 1\n",
    "            if prev_ti is not None and prev_prev_ti is not None:\n",
    "                tag_seqs[ti, prev_ti, prev_prev_ti] += 1\n",
    "            prev_prev_ti = prev_ti\n",
    "            prev_ti = ti\n",
    "        self.word_tag_probs = word_tags / word_tags.sum(axis=0).astype(np.float32)\n",
    "        self.tag_seq_probs = tag_seqs / tag_seqs.sum(axis=(1,2)).astype(np.float32)\n",
    "        # enable prediction\n",
    "        self.trained = True\n",
    "        print(\"trained on {} words and {} tags\".format(self.W, self.T))\n",
    "        return\n",
    "    \n",
    "    def _step(self, viterbi_prev, p_tag, tag, wi):\n",
    "            m, arg_m = 0., -1  # 0 is a valid index\n",
    "            word_tag_prob = self.word_tag_probs[wi, tag]\n",
    "            if word_tag_prob == 0:\n",
    "                return 0., 0\n",
    "            for pp_tag in range(self.T):\n",
    "                # print('word: {}'.format(self.doiw[wi]))\n",
    "                # print('tested tag seq: {}, {}, {}'.format(self.doit[pp_tag], self.doit[p_tag], self.doit[tag]))\n",
    "                prod = (viterbi_prev[pp_tag, p_tag] * \n",
    "                        self.tag_seq_probs[tag, p_tag, pp_tag] * \n",
    "                        word_tag_prob)\n",
    "                # print('prev[{}, {}] = {}'.format(pp_tag, p_tag, viterbi_prev[pp_tag, p_tag]))\n",
    "                # print('tag_seq_prob: {}'.format(self.tag_seq_probs[tag, p_tag, pp_tag]))\n",
    "                # print('word_tag_prob: {}'.format(word_tag_prob))\n",
    "                # print('pi: {}'.format(prod))\n",
    "                if prod > m:\n",
    "                    m, arg_m = prod, pp_tag\n",
    "                    print('new max found: {} for word {}, tag {}'.format(m, self.doiw[wi], self.doit[arg_m]))\n",
    "            return m, arg_m\n",
    "        \n",
    "    def predict(self, words):\n",
    "        # raise error if not trained yet    \n",
    "        if not self.trained:\n",
    "            raise NotTrainedException(\"Train me first!\")\n",
    "        # viterbi init\n",
    "        N = len(words)  # length of sentence\n",
    "        viterbi = np.zeros((N, self.T, self.T), dtype=np.float32)\n",
    "        backpts = -np.ones((N, self.T, self.T), dtype=np.int32)  # 0 is a valid index\n",
    "        # k=0 -> pi(0, *, *) = 1\n",
    "        viterbi[0, :, :] = np.ones((self.T, self.T))\n",
    "        # k=1 -> prev_prev_tag does not exist\n",
    "        # set pi(1, *, v) = P(w_1|v)\n",
    "        wi = self.dowi[words[1]]\n",
    "        for tag in range(self.T):\n",
    "            viterbi[1, :, tag] = self.word_tag_probs[wi, tag] * np.ones((self.T))\n",
    "        # k=2...N\n",
    "        for k in range(2, N):\n",
    "            wi = self.dowi[words[k]]\n",
    "            for tag in range(self.T):\n",
    "                for prev_tag in range(self.T):\n",
    "                    viterbi[k, prev_tag, tag], backpts[k, prev_tag, tag] = self._step(viterbi[k-1, :, :], prev_tag, tag, wi)\n",
    "        # get max likelihood last tags\n",
    "        print(backpts)\n",
    "        max_prob = viterbi[-1, :, :].max()\n",
    "        max_prob_tags = np.where(viterbi[-1, :, :] == max_prob)\n",
    "        next_tag = max_prob_tags[0][0]\n",
    "        next_next_tag = max_prob_tags[1][0]\n",
    "        pred_tags_reversed = [self.doit[next_next_tag], self.doit[next_tag]]\n",
    "        print(pred_tags_reversed)\n",
    "        # backtrack max likelihood tags\n",
    "        for k in range(N-1, 2, -1):\n",
    "            pred_current_tag = backpts[k, next_tag, next_next_tag]\n",
    "            pred_tags_reversed.append(self.doit[pred_current_tag])\n",
    "            next_next_tag = next_tag\n",
    "            next_tag = pred_current_tag\n",
    "            print(pred_tags_reversed)\n",
    "        # flip predicted tags\n",
    "        pred_tags_reversed.reverse()\n",
    "        return max_prob,  pred_tags_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    generator1 = (line.strip().split(\"\\t\") for line in f)\n",
    "    generator2 = (line for line in generator1 if len(line) == 2)\n",
    "    \n",
    "    viterbi = Viterbi()\n",
    "    viterbi.train(generator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {'the': 'DT',\n",
    "        'cat': 'NN', 'dog': 'NN', 'man': 'NN',\n",
    "        'goes': 'VBZ', 'sits': 'VBZ',\n",
    "        'to': 'TO', 'on': 'IN', \n",
    "        'store': 'NN', 'chair': 'NN', 'bed': 'NN',\n",
    "        '.': '.',\n",
    "       }\n",
    "sents = ('the cat goes to the store .',\n",
    "         'the cat sits on the bed .',\n",
    "         'the cat sits on the chair .',\n",
    "         'the dog goes to the store .',\n",
    "         'the dog sits on the bed .',\n",
    "         'the dog sits on the chair .',\n",
    "         'the man goes to the store .',\n",
    "         'the man sits on the bed .',\n",
    "         # 'the man sits on the chair .',\n",
    "        )\n",
    "dummy = [(word, tags[word]) for sent in sents for word in sent.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained on 12 words and 6 tags\n"
     ]
    }
   ],
   "source": [
    "v = Viterbi()\n",
    "v.train(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new max found: 0.041666666666666664 for word sits, tag DT\n",
      "new max found: 0.013888889302810032 for word on, tag NN\n",
      "new max found: 0.008680555620230734 for word the, tag VBZ\n",
      "new max found: 0.0010850694961845875 for word chair, tag IN\n",
      "new max found: 0.0005787037312984467 for word ., tag DT\n",
      "[[[-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]]\n",
      "\n",
      " [[-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1 -1 -1]]\n",
      "\n",
      " [[ 0  0 -1  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0 -1  0  0  0]\n",
      "  [ 0  0 -1  0  0  0]\n",
      "  [ 0  0 -1  0  0  0]\n",
      "  [ 0  0 -1  0  0  0]]\n",
      "\n",
      " [[ 0  0  0  0  0 -1]\n",
      "  [ 0  0  0  0  0 -1]\n",
      "  [ 0  0  0  0  0  1]\n",
      "  [ 0  0  0  0  0 -1]\n",
      "  [ 0  0  0  0  0 -1]\n",
      "  [ 0  0  0  0  0 -1]]\n",
      "\n",
      " [[-1  0  0  0  0  0]\n",
      "  [-1  0  0  0  0  0]\n",
      "  [-1  0  0  0  0  0]\n",
      "  [-1  0  0  0  0  0]\n",
      "  [-1  0  0  0  0  0]\n",
      "  [ 2  0  0  0  0  0]]\n",
      "\n",
      " [[ 0  5  0  0  0  0]\n",
      "  [ 0 -1  0  0  0  0]\n",
      "  [ 0 -1  0  0  0  0]\n",
      "  [ 0 -1  0  0  0  0]\n",
      "  [ 0 -1  0  0  0  0]\n",
      "  [ 0 -1  0  0  0  0]]\n",
      "\n",
      " [[ 0  0  0  0 -1  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0 -1  0]\n",
      "  [ 0  0  0  0 -1  0]\n",
      "  [ 0  0  0  0 -1  0]\n",
      "  [ 0  0  0  0 -1  0]]]\n",
      "['.', 'NN']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-8f79d070109c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'the man sits on the chair .'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-111-426637ef3ff2>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# backtrack max likelihood tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mpred_current_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackpts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_next_tag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mpred_tags_reversed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred_current_tag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mnext_next_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 7 is out of bounds for axis 0 with size 7"
     ]
    }
   ],
   "source": [
    "v.predict('the man sits on the chair .'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(viterbi.predict(\"You talk the talk .\".split()) == ['PRP', 'VB', 'DT', 'NN', '.'])\n",
    "print(viterbi.predict(\"The dog .\".split()))\n",
    "print(viterbi.predict(\"The dog runs .\".split()))\n",
    "print(viterbi.predict(\"The dog runs slowly .\".split()))\n",
    "print(viterbi.predict(\"The dog 's run was slow .\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small exercise 1. (10 points)\n",
    "Modify the Viterbi class: use a logarithmic scale for probabilities.\n",
    "\n",
    "In the Viterbi table instead of \n",
    "$$\n",
    "\\pi(k,u,v) = \\max_{w\\in L} \\pi(k-1, w, u)\\cdot \\mathbb{P}(v \\ | \\ w,u)\\cdot \\mathbb{P}(w_k \\ | \\ v) \n",
    "$$\n",
    "use\n",
    "$$\n",
    "\\hat\\pi(k,u,v) = \\max_{w\\in L} \\hat\\pi(k−1,w,u) + \\log\\mathbb{P}(v \\ | \\ w,u) + \\log\\mathbb{P}(w_k \\ | \\ v) \n",
    "$$\n",
    "\n",
    "Note that the minimum probability is $0$, but the minimum logarithm is $-\\infty$. Both numpy and python float can deal with minus infinity.<br>\n",
    "Precalculate the log-probabilities in the initializer, not during the dymanic programming.\n",
    "\n",
    "This should not affect the result, just the numbers in the viterbi table.\n",
    "\n",
    "Name the log-scaled imlementation `ViterbiLog`, it can inherit from `Viterbi` or it can be a whole new class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    generator1 = (line.strip().split(\"\\t\") for line in f)\n",
    "    generator2 = (line for line in generator1 if len(line) == 2)\n",
    "\n",
    "    viterbi_log = ViterbiLog()\n",
    "    viterbi_log.train(generator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(viterbi.predict(\"The dog runs slowly .\".split()) == viterbi_log.predict(\"The dog runs slowly .\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small exercise 2. (30 points)\n",
    "### a) 15 points\n",
    "Modify the Viterbi class: use a sparse storage for transition probabilities, not a 3-dimensional array.\n",
    "\n",
    "Use a dict to store the frequencies of the 2 and 3 tuples of labels.\n",
    "\n",
    "For example if you had _\"adjective noun\"_ 10 times and _\"adjective noun determinant\"_ 5 times, then store the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{('JJ', 'NN'): 10, ('JJ', 'NN', 'DT'): 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example $\\mathbb{P}(DT \\ | \\ JJ, NN ) = 0.5$\n",
    "\n",
    "Note that whenever $\\#\\{JJ, NN\\} = 0$ or $\\#\\{JJ, NN, DT\\} = 0$, then $\\mathbb{P}(DT \\ | \\ JJ, NN ) = 0$.\n",
    "\n",
    "Implement this in a new class `ViterbiSparse`, it can inherit from the original one or it can be a new class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) 15 points\n",
    "Try to find a sparse representation (with `dict`-s) which makes the inner for loop shorter. Note that you don't have to take the maximum over all the $w\\in L$ elements, if you already know that some transition probabilities are zeros.\n",
    "\n",
    "$$\n",
    "\\max_{\\substack{w\\in L \\\\ \\mathbb{P}(v \\ | \\ w,u) > 0}} \\pi(k-1, w, u)\\cdot \\mathbb{P}(v \\ | \\ w,u)\\cdot \\mathbb{P}(w_k \\ | \\ v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
