{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Katona Máté (PD6YOR)\n",
    "\n",
    "\n",
    "# Homework 2\n",
    "\n",
    "The maximum score of this homework is 100 points. Grading is listed in this table:\n",
    "\n",
    "| Grade | Score range |\n",
    "| --- | --- |\n",
    "| 5 | 85+ |\n",
    "| 4 | 70-84 |\n",
    "| 3 | 55-69 |\n",
    "| 2 | 40-54 |\n",
    "| 1 | 0-39 |\n",
    "\n",
    "Most exercises include tests which should pass if your solution is correct.\n",
    "However successful test do not guarantee that your solution is correct.\n",
    "You are free to add more tests.\n",
    "\n",
    "## Deadline\n",
    "__2017 November 20<sup>th</sup> Monday 23:59__\n",
    "\n",
    "\n",
    "# Main exercise (60 points)\n",
    "\n",
    "Implement the Viterbi algorithm fot $k=3$.\n",
    "\n",
    "The input can be found [here](http://sandbox.hlt.bme.hu/~gaebor/ea_anyag/python_nlp/).\n",
    "You can use the 1, 10 or 100 million word corpus, it is advised to use the 1 million corpus while you are developing and you can try te larger ones afterwards.\n",
    "\n",
    "Write a class called `Viterbi` with the following attributes:\n",
    "* `__init__`: has no arguments, except `self`\n",
    "* `train`: one argument (aside `self`), an iterable object which generates 2-tuples of strings `[(word1, pos1), (word2, pos2), ...]`\n",
    "  * initializes the vocabularies, empirical probabilities or any other data attributes needed for the algorithm.\n",
    "  * you can read the data (generator) only once\n",
    "  * returns `None`\n",
    "* predict: one argument (aside `self`), an iterable object, a list of words.\n",
    "  * returns the predicted label sequence: a list of labels with the same length as the input.\n",
    "\n",
    "Don't use global variables!\n",
    "\n",
    "### Hint\n",
    "* Use a 3-dimensional array for the transition probabilities $\\mathbb{P}(v \\ | \\ w, u)$.\n",
    "* Use a 3-dimensional array for the Viterbi table, one index for time, one index for the previous state and one index for the state before that.\n",
    "* Same for the backtracking table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotTrainedException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Viterbi(object):\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "        \n",
    "    def train(self, data):\n",
    "        wt, ts, self.wd2id, self.tg2id = {}, {}, {}, {}\n",
    "        prev_ti, prev_prev_ti = None, None\n",
    "        # process data: word and tag ids, count tags and tag sequences\n",
    "        for w, t in data:\n",
    "            self.wd2id[w] = self.wd2id.get(w, len(self.wd2id))\n",
    "            self.tg2id[t] = self.tg2id.get(t, len(self.tg2id))\n",
    "            wi = self.wd2id[w]  # id of word\n",
    "            ti = self.tg2id[t]  # id of tag\n",
    "            wt[wi] = wt.get(wi, {})\n",
    "            wt[wi][ti] = wt[wi].get(ti, 0) + 1  # count word w tag t\n",
    "            if prev_ti is not None and prev_prev_ti is not None:\n",
    "                seq = (ti, prev_ti, prev_prev_ti)\n",
    "                ts[seq] = ts.get(seq, 0) + 1  # count tag sequence (ppt, pt, t)\n",
    "            prev_prev_ti = prev_ti\n",
    "            prev_ti = ti\n",
    "        self.id2wd = {wi: w for w, wi in self.wd2id.items()}\n",
    "        self.id2tg = {ti: t for t, ti in self.tg2id.items()}\n",
    "        # create tables from dicts (now that size is known)\n",
    "        self.W = len(self.wd2id)  # number of words\n",
    "        self.T = len(self.tg2id)  # number of tags\n",
    "        word_tags = np.zeros((self.W, self.T), dtype=np.int32)  # number of (word, tag) occurences\n",
    "        tag_seqs = np.zeros((self.T, self.T, self.T), dtype=np.int32)  # number of (tag, tag, tag) occurences\n",
    "        for word, tags in wt.items():\n",
    "            for tag, count in tags.items():\n",
    "                word_tags.itemset((word, tag), count)\n",
    "        for seq, count in ts.items():\n",
    "            tag_seqs.itemset(seq, count)\n",
    "        # convert nr of occurences to probabilities\n",
    "        self.convert_probs(word_tags, tag_seqs)\n",
    "        # enable prediction\n",
    "        self.trained = True\n",
    "        print(\"trained on {} words and {} tags\".format(self.W, self.T))\n",
    "        return\n",
    "    \n",
    "    def convert_probs(self, word_tags, tag_seqs):\n",
    "        self.word_tag_probs = word_tags / word_tags.sum(axis=0).astype(np.float32)\n",
    "        self.tag_seq_probs = tag_seqs / tag_seqs.sum(axis=(1,2)).astype(np.float32)\n",
    "        return\n",
    "    \n",
    "    def _step(self, k_prev, p_tag, tag, word_tag_prob):\n",
    "            m, arg_m = 0., -1  # 0 is a valid tag\n",
    "            for pp_tag in range(self.T):\n",
    "                prod = (self.viterbi[k_prev, pp_tag, p_tag] * \n",
    "                        self.tag_seq_probs[tag, p_tag, pp_tag] * \n",
    "                        word_tag_prob)\n",
    "                if prod > m:\n",
    "                    m, arg_m = prod, pp_tag\n",
    "            return m, arg_m\n",
    "        \n",
    "    def predict(self, words):\n",
    "        # raise error if not trained yet    \n",
    "        if not self.trained:\n",
    "            raise NotTrainedException(\"Train me first!\")\n",
    "        # viterbi init\n",
    "        N = len(words)  # length of sentence\n",
    "        self.viterbi = np.zeros((N, self.T, self.T), dtype=np.float32)\n",
    "        self.backpts = -np.ones((N-2, self.T, self.T), dtype=np.int32)  # 0 is a valid index\n",
    "        # k=0 -> (prev_)prev_tag does not exist\n",
    "        # set viterbi(0, *, v) = P(w_0|v)\n",
    "        wi = self.wd2id[words[0]]\n",
    "        for tag in range(self.T):\n",
    "            self.viterbi[0, :, tag] = self.word_tag_probs[wi, tag] * np.ones((self.T))\n",
    "        # k=1 -> prev_prev_tag does not exist\n",
    "        # set viterbi(1, u, v) = P(w_1|u) * P(u|v)\n",
    "        wi = self.wd2id[words[1]]\n",
    "        for tag in range(self.T):\n",
    "            word_tag_prob = self.word_tag_probs[wi, tag]\n",
    "            if word_tag_prob == 0:\n",
    "                self.viterbi[k, :, tag] = np.zeros((self.T))\n",
    "                self.backpts[k-2, :, tag] = -np.ones((self.T))  # zero is a valid tag\n",
    "                continue\n",
    "            for prev_tag in range(self.T):\n",
    "                tag_seq_prob = self.tag_seq_probs[tag, prev_tag, :].sum()\n",
    "                self.viterbi[1, prev_tag, tag] = word_tag_prob * tag_seq_prob\n",
    "        # k=2...N -> find most likely prev_prev_tag\n",
    "        for k in range(2, N):\n",
    "            wi = self.wd2id[words[k]]\n",
    "            for tag in range(self.T):\n",
    "                word_tag_prob = self.word_tag_probs[wi, tag]\n",
    "                if word_tag_prob == 0:\n",
    "                    self.viterbi[k, :, tag] = np.zeros((self.T))\n",
    "                    self.backpts[k-2, :, tag] = -np.ones((self.T))  # zero is a valid tag\n",
    "                    continue\n",
    "                for prev_tag in range(self.T):\n",
    "                    p, t = self._step(k-1, prev_tag, tag, word_tag_prob)\n",
    "                    self.viterbi[k, prev_tag, tag] = p  # to make lines shorter\n",
    "                    self.backpts[k-2, prev_tag, tag] = t\n",
    "        # get max likelihood last tags\n",
    "        max_prob = self.viterbi[-1, :, :].max()\n",
    "        max_prob_tags = np.where(self.viterbi[-1, :, :] == max_prob)\n",
    "        next_tag = max_prob_tags[0][0]\n",
    "        next_next_tag = max_prob_tags[1][0]\n",
    "        pred_tags = deque([self.id2tg[next_tag], self.id2tg[next_next_tag]], maxlen=N)\n",
    "        # backtrack max likelihood tags\n",
    "        for k in range(N-2-1, -1, -1):\n",
    "            pred_current_tag = self.backpts[k, next_tag, next_next_tag]\n",
    "            pred_tags.appendleft(self.id2tg[pred_current_tag])\n",
    "            next_next_tag = next_tag\n",
    "            next_tag = pred_current_tag\n",
    "        return list(pred_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = {'the': 'DT',\n",
    "        'cat': 'NN', 'dog': 'NN', 'man': 'NN',\n",
    "        'goes': 'VBZ', 'sits': 'VBZ',\n",
    "        'to': 'TO', 'on': 'IN', \n",
    "        'store': 'NN', 'chair': 'NN', 'bed': 'NN',\n",
    "        '.': '.',\n",
    "       }\n",
    "sents = ('the cat goes to the store .',\n",
    "         'the cat sits on the bed .',\n",
    "         'the cat sits on the chair .',\n",
    "         'the dog goes to the store .',\n",
    "         'the dog sits on the bed .',\n",
    "         'the dog sits on the chair .',\n",
    "         'the man goes to the store .',\n",
    "         'the man sits on the bed .',\n",
    "         # 'the man sits on the chair .',\n",
    "        )\n",
    "dummy = [(word, tags[word]) for sent in sents for word in sent.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = Viterbi()\n",
    "vd.train(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd.predict('the man sits on the chair .'.split(' '))\n",
    "print(vd.id2wd)\n",
    "print(vd.id2tg)\n",
    "print(vd.word_tag_probs)\n",
    "print(vd.tag_seq_probs)\n",
    "print(vd.viterbi)\n",
    "print(vd.backpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Viterbi()\n",
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    generator1 = (line.strip().split(\"\\t\") for line in f)\n",
    "    generator2 = (line for line in generator1 if len(line) == 2)\n",
    "    v.train(generator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v.predict(\"You talk the talk .\".split()))\n",
    "#assert(v.predict(\"You talk the talk .\".split()) == ['PRP', 'VB', 'DT', 'NN', '.'])\n",
    "print(v.predict(\"The dog .\".split()))\n",
    "print(v.predict(\"The dog runs .\".split()))\n",
    "print(v.predict(\"The dog runs slowly .\".split()))\n",
    "print(v.predict(\"The dog 's run was slow .\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small exercise 1. (10 points)\n",
    "Modify the Viterbi class: use a logarithmic scale for probabilities.\n",
    "\n",
    "In the Viterbi table instead of \n",
    "$$\n",
    "\\pi(k,u,v) = \\max_{w\\in L} \\pi(k-1, w, u)\\cdot \\mathbb{P}(v \\ | \\ w,u)\\cdot \\mathbb{P}(w_k \\ | \\ v) \n",
    "$$\n",
    "use\n",
    "$$\n",
    "\\hat\\pi(k,u,v) = \\max_{w\\in L} \\hat\\pi(k−1,w,u) + \\log\\mathbb{P}(v \\ | \\ w,u) + \\log\\mathbb{P}(w_k \\ | \\ v) \n",
    "$$\n",
    "\n",
    "Note that the minimum probability is $0$, but the minimum logarithm is $-\\infty$. Both numpy and python float can deal with minus infinity.<br>\n",
    "Precalculate the log-probabilities in the initializer, not during the dymanic programming.\n",
    "\n",
    "This should not affect the result, just the numbers in the viterbi table.\n",
    "\n",
    "Name the log-scaled imlementation `ViterbiLog`, it can inherit from `Viterbi` or it can be a whole new class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViterbiLog(Viterbi):\n",
    "    def convert_probs(self, word_tags, tag_seqs):\n",
    "        super().convert_probs(word_tags, tag_seqs)\n",
    "        self.word_tag_probs = np.log(self.word_tag_probs)\n",
    "        self.tag_seq_probs = np.log(self.tag_seq_probs)\n",
    "        return\n",
    "    \n",
    "    def _step(self, k_prev, p_tag, tag, word_tag_prob):\n",
    "        m, arg_m = 0., -1  # 0 is a valid tag\n",
    "        for pp_tag in range(self.T):\n",
    "            prod = (self.viterbi[k_prev, pp_tag, p_tag] + \n",
    "                    self.tag_seq_probs[tag, p_tag, pp_tag] + \n",
    "                    word_tag_prob)\n",
    "            if prod > m:\n",
    "                m, arg_m = prod, pp_tag\n",
    "        return m, arg_m\n",
    "        \n",
    "    def predict(self, words):\n",
    "        # raise error if not trained yet    \n",
    "        if not self.trained:\n",
    "            raise NotTrainedException(\"Train me first!\")\n",
    "        # viterbi init\n",
    "        N = len(words)  # length of sentence\n",
    "        self.viterbi = np.zeros((N, self.T, self.T), dtype=np.float32)\n",
    "        self.backpts = -np.ones((N-2, self.T, self.T), dtype=np.int32)  # 0 is a valid index\n",
    "        # k=0 -> (prev_)prev_tag does not exist\n",
    "        # set viterbi(0, *, v) = log P(w_0|v)\n",
    "        wi = self.wd2id[words[0]]\n",
    "        for tag in range(self.T):\n",
    "            self.viterbi[0, :, tag] = self.word_tag_probs[wi, tag] * np.ones((self.T))\n",
    "        # k=1 -> prev_prev_tag does not exist\n",
    "        # set viterbi(1, u, v) = log P(w_1|u) + log P(u|v)\n",
    "        wi = self.wd2id[words[1]]\n",
    "        for tag in range(self.T):\n",
    "            word_tag_prob = self.word_tag_probs[wi, tag]\n",
    "            if word_tag_prob == -np.inf:\n",
    "                    self.viterbi[1, :, tag] = np.log(np.zeros((self.T)))\n",
    "                    continue\n",
    "            for prev_tag in range(self.T):\n",
    "                tag_seq_prob = self.tag_seq_probs[tag, prev_tag, :].sum()\n",
    "                self.viterbi[1, prev_tag, tag] = word_tag_prob + tag_seq_prob\n",
    "        # k=2...N -> find most likely prev_prev_tag\n",
    "        for k in range(2, N):\n",
    "            wi = self.wd2id[words[k]]\n",
    "            for tag in range(self.T):\n",
    "                word_tag_prob = self.word_tag_probs[wi, tag]\n",
    "                if word_tag_prob == - np.inf:\n",
    "                    self.viterbi[k, :, tag] = np.log(np.zeros((self.T)))\n",
    "                    self.backpts[k-2, :, tag] = -np.ones((self.T))  # zero is a valid tag\n",
    "                    continue\n",
    "                for prev_tag in range(self.T):\n",
    "                    p, t = self._step(k-1, prev_tag, tag, word_tag_prob)\n",
    "                    self.viterbi[k, prev_tag, tag] = p  # to make lines shorter\n",
    "                    self.backpts[k-2, prev_tag, tag] = t\n",
    "        print(self.viterbi)\n",
    "        # get max likelihood last tags\n",
    "        max_prob = self.viterbi[-1, :, :].max()\n",
    "        max_prob_tags = np.where(self.viterbi[-1, :, :] == max_prob)\n",
    "        next_tag = max_prob_tags[0][0]\n",
    "        next_next_tag = max_prob_tags[1][0]\n",
    "        pred_tags = deque([self.id2tg[next_tag], self.id2tg[next_next_tag]], maxlen=N)\n",
    "        # backtrack max likelihood tags\n",
    "        for k in range(N-2-1, -1, -1):\n",
    "            pred_current_tag = self.backpts[k, next_tag, next_next_tag]\n",
    "            pred_tags.appendleft(self.id2tg[pred_current_tag])\n",
    "            next_next_tag = next_tag\n",
    "            next_tag = pred_current_tag\n",
    "        return list(pred_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    generator1 = (line.strip().split(\"\\t\") for line in f)\n",
    "    generator2 = (line for line in generator1 if len(line) == 2)\n",
    "\n",
    "    viterbi_log = ViterbiLog()\n",
    "    viterbi_log.train(generator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert(viterbi.predict(\"The dog runs slowly .\".split()) == viterbi_log.predict(\"The dog runs slowly .\".split()))\n",
    "print(v.predict(\"The dog runs slowly .\".split()))\n",
    "print(viterbi_log.predict(\"The dog runs slowly .\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = ViterbiLog()\n",
    "vd.train(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vd.predict('the man sits on the chair .'.split(' '))\n",
    "print(vd.id2wd)\n",
    "print(vd.id2tg)\n",
    "print(vd.word_tag_probs)\n",
    "print(vd.tag_seq_probs)\n",
    "print(vd.viterbi)\n",
    "print(vd.backpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small exercise 2. (30 points)\n",
    "### a) 15 points\n",
    "Modify the Viterbi class: use a sparse storage for transition probabilities, not a 3-dimensional array.\n",
    "\n",
    "Use a dict to store the frequencies of the 2 and 3 tuples of labels.\n",
    "\n",
    "For example if you had _\"adjective noun\"_ 10 times and _\"adjective noun determinant\"_ 5 times, then store the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{('JJ', 'NN'): 10, ('JJ', 'NN', 'DT'): 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example $\\mathbb{P}(DT \\ | \\ JJ, NN ) = 0.5$\n",
    "\n",
    "Note that whenever $\\#\\{JJ, NN\\} = 0$ or $\\#\\{JJ, NN, DT\\} = 0$, then $\\mathbb{P}(DT \\ | \\ JJ, NN ) = 0$.\n",
    "\n",
    "Implement this in a new class `ViterbiSparse`, it can inherit from the original one or it can be a new class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) 15 points\n",
    "Try to find a sparse representation (with `dict`-s) which makes the inner for loop shorter. Note that you don't have to take the maximum over all the $w\\in L$ elements, if you already know that some transition probabilities are zeros.\n",
    "\n",
    "$$\n",
    "\\max_{\\substack{w\\in L \\\\ \\mathbb{P}(v \\ | \\ w,u) > 0}} \\pi(k-1, w, u)\\cdot \\mathbb{P}(v \\ | \\ w,u)\\cdot \\mathbb{P}(w_k \\ | \\ v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
