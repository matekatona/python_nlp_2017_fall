{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Semantics 2 - Lab excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving a baseline Sentiment Analysis algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a small system for training and testing a Support Vector classifier on sentiment analysis data from the 2017 Semeval Task 4a, containing English tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the system only contains a single feature type: each tweet is represented by the set of words it contains. More specifically, a binary feature is created for each word in the vocabulary of the full training set, and the value of each feature for any given tweet is 1 if the word is present and 0 otherwise.\n",
    "\n",
    "Your task will be to improve the performance of the system by implementing other binary features. (If you want to include non-binary features, you will also have to change the provided code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-12-08 11:28:00--  http://sandbox.hlt.bme.hu/~recski/stuff/4a.tgz\n",
      "Resolving sandbox.hlt.bme.hu (sandbox.hlt.bme.hu)... 152.66.88.21\n",
      "Connecting to sandbox.hlt.bme.hu (sandbox.hlt.bme.hu)|152.66.88.21|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1457736 (1.4M) [application/x-gzip]\n",
      "Saving to: '4a.tgz'\n",
      "\n",
      "4a.tgz              100%[===================>]   1.39M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2017-12-08 11:28:00 (10.9 MB/s) - '4a.tgz' saved [1457736/1457736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://sandbox.hlt.bme.hu/~recski/stuff/4a.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- recski/recski 381255 2017-08-15 13:27 4a.dev\n",
      "-rw-rw-r-- recski/recski 2613689 2017-08-15 13:27 4a.train\n",
      "-rw-rw-r-- recski/recski   14227 2017-08-16 10:21 test.dev\n",
      "-rw-rw-r-- recski/recski  145899 2017-08-16 10:20 test.train\n"
     ]
    }
   ],
   "source": [
    "!tar xvvf 4a.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4a.train__ and __4a.dev__ are the full datasets for training and testing, __test.train__ and __test.dev__ are small samples from these that you may want to use while debugging your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you get started, let's walk through the main components of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Featurizer__ class implements features as static methods and also converts train and test data to data structures handled by __sklearn__, the library we use for training an SVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "class Featurizer():\n",
    "    @staticmethod\n",
    "    def bag_of_words(text):\n",
    "        # yield (word.lower() for word in word_tokenize(text))\n",
    "        for word in word_tokenize(text):\n",
    "            yield word.lower()\n",
    "            \n",
    "    @staticmethod\n",
    "    def question(text):\n",
    "        if '?' in text:\n",
    "            yield 'question'\n",
    "\n",
    "    @staticmethod\n",
    "    def exclam(text):\n",
    "        if '!' in text:\n",
    "            yield 'exclam'\n",
    "\n",
    "    @staticmethod\n",
    "    def dots(text):\n",
    "        if '...?' in text:\n",
    "            yield 'dots'\n",
    "            \n",
    "    @staticmethod\n",
    "    def reply(text):\n",
    "        if '@' in text:\n",
    "            yield 'reply'\n",
    "    \n",
    "    @staticmethod\n",
    "    def lastc(text):\n",
    "        yield 'last_{}'.format(text[-1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def ngram(text, n=4):\n",
    "        words = word_tokenize(text.lower())\n",
    "        for i in range(len(words)):\n",
    "            if i < len(words) - n:\n",
    "                yield \" \".join(words[i:i+n])\n",
    "    \n",
    "    @staticmethod\n",
    "    def num_of_words(text):\n",
    "        yield 'numwords_{}'.format(len(word_tokenize(text)))\n",
    "    \n",
    "    feature_functions = [\n",
    "        'bag_of_words', \n",
    "        # 'question',\n",
    "        # 'reply',\n",
    "        'lastc',\n",
    "        # 'num_of_e',\n",
    "        'ngram',\n",
    "        'num_of_words',\n",
    "        #'exclam',\n",
    "        'dots',\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.labels = {}\n",
    "        self.labels_by_id = {}\n",
    "        self.features = {}\n",
    "        self.features_by_id = {}\n",
    "        self.next_feature_id = 0\n",
    "        self.next_label_id = 0\n",
    "\n",
    "    def to_sparse(self, events):\n",
    "        \"\"\"convert sets of ints to a scipy.sparse.csr_matrix\"\"\"\n",
    "        data, row_ind, col_ind = [], [], []\n",
    "        for event_index, event in enumerate(events):\n",
    "            for feature in event:\n",
    "                data.append(1)\n",
    "                row_ind.append(event_index)\n",
    "                col_ind.append(feature)\n",
    "\n",
    "        n_features = self.next_feature_id\n",
    "        n_events = len(events)\n",
    "        matrix = scipy.sparse.csr_matrix(\n",
    "            (data, (row_ind, col_ind)), shape=(n_events, n_features))\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def featurize(self, dataset, allow_new_features=False):\n",
    "        events, labels = [], []\n",
    "        n_events = len(dataset)\n",
    "        for c, (text, label) in enumerate(dataset):\n",
    "            if c % 2000 == 0:\n",
    "                print(\"{0:.0%}...\".format(c/n_events), end='')\n",
    "            if label not in self.labels:\n",
    "                self.labels[label] = self.next_label_id\n",
    "                self.labels_by_id[self.next_label_id] = label\n",
    "                self.next_label_id += 1\n",
    "            labels.append(self.labels[label])\n",
    "            events.append(set())\n",
    "            for function_name in Featurizer.feature_functions:\n",
    "                function = getattr(Featurizer, function_name)\n",
    "                for feature in function(text):\n",
    "                    if feature not in self.features:\n",
    "                        if not allow_new_features:\n",
    "                            continue\n",
    "                        self.features[feature] = self.next_feature_id\n",
    "                        self.features_by_id[self.next_feature_id] = feature\n",
    "                        self.next_feature_id += 1\n",
    "                    feat_id = self.features[feature]\n",
    "                    events[-1].add(feat_id)\n",
    "        \n",
    "        print('done, sparsifying...', end='')\n",
    "        events_sparse = self.to_sparse(events)\n",
    "        labels_array = np.array(labels)\n",
    "        print('done!')\n",
    "\n",
    "        return events_sparse, labels_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to evaluate our output against the gold data, using the metrics defined for the competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def evaluate(predictions, dev_labels):\n",
    "    stats_by_label = defaultdict(lambda: defaultdict(int))\n",
    "    for i, gold in enumerate(dev_labels):\n",
    "        auto = predictions[i]\n",
    "        # print(auto, gold)\n",
    "        if auto == gold:\n",
    "            stats_by_label[auto]['tp'] += 1\n",
    "        else:\n",
    "            stats_by_label[auto]['fp'] += 1\n",
    "            stats_by_label[gold]['fn'] += 1\n",
    "\n",
    "    print(\"{:>8} {:>8} {:>8} {:>8} {:>8} {:>8}\".format(\n",
    "        'label', 'n_true', 'n_tagged', 'precision', 'recall', 'F-score'))\n",
    "    for label, stats in stats_by_label.items():\n",
    "        all_tagged = stats['tp'] + stats['fp']\n",
    "        stats['prec'] = stats['tp'] / all_tagged if all_tagged else 0\n",
    "        all_true = stats['tp'] + stats['fn']\n",
    "        stats['rec'] = stats['tp'] / all_true if all_true else 0\n",
    "        stats['f'] = (2 / ((1/stats['prec']) + (1/stats['rec']))\n",
    "                      if stats['prec'] > 0 and stats['rec'] > 0 else 0)\n",
    "\n",
    "        print(\"{:>8} {:>8} {:>8} {:>8.2f} {:>8.2f} {:>8.2f}\".format(\n",
    "            label, all_true, all_tagged, stats['prec'], stats['rec'],\n",
    "            stats['f']))\n",
    "\n",
    "    accuracy = (\n",
    "        sum([stats_by_label[label]['tp'] for label in stats_by_label]) /\n",
    "        len(predictions)) if predictions else 0\n",
    "\n",
    "    av_rec = sum([stats['rec'] for stats in stats_by_label.values()]) / 3\n",
    "    f_pn = (stats_by_label['positive']['f'] +\n",
    "            stats_by_label['negative']['f']) / 2\n",
    "\n",
    "    print()\n",
    "    print(\"{:>10} {:>.4f}\".format('Acc:', accuracy))\n",
    "    print(\"{:>10} {:>.4f}\".format('P/N av. F:', f_pn))\n",
    "    print(\"{:>10} {:>.4f}\".format('Av.rec:', av_rec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a small function to read the data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def read_data(fn):\n",
    "    data = []\n",
    "    with open(fn) as f:\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            fields = line.strip().split('\\t')\n",
    "            if line.strip() == '\"':\n",
    "                continue\n",
    "            answer, text = fields[1:3]\n",
    "            data.append((text, answer))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally a main function to run an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def sa_exp(train_file, dev_file):\n",
    "    print('reading data...')\n",
    "    train_data = read_data(train_file)\n",
    "    dev_data = read_data(dev_file)\n",
    "\n",
    "    print('featurizing train...')\n",
    "    featurizer = Featurizer()\n",
    "    train_events, train_labels = featurizer.featurize(\n",
    "        train_data, allow_new_features=True)\n",
    "    print('featurizing dev...')\n",
    "    dev_events, dev_labels = featurizer.featurize(\n",
    "        dev_data, allow_new_features=False)\n",
    "\n",
    "    print('training...')\n",
    "    model = svm.LinearSVC()\n",
    "    model.fit(train_events, train_labels)\n",
    "\n",
    "    print('predicting...')\n",
    "    predictions = model.predict(dev_events)\n",
    "    predicted_labels = [\n",
    "        featurizer.labels_by_id[label] for label in predictions]\n",
    "\n",
    "    dev_labels = [\n",
    "        featurizer.labels_by_id[label] for label in dev_labels]\n",
    "\n",
    "    print('evaluating...')\n",
    "    print()\n",
    "    evaluate(predicted_labels, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the system performs currently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "featurizing train...\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...89%...done, sparsifying...done!\n",
      "featurizing dev...\n",
      "0%...76%...done, sparsifying...done!\n",
      "training...\n",
      "predicting...\n",
      "evaluating...\n",
      "\n",
      "   label   n_true n_tagged precision   recall  F-score\n",
      " neutral     1338     1558     0.68     0.79     0.73\n",
      "positive      864      855     0.68     0.67     0.68\n",
      "negative      430      219     0.63     0.32     0.43\n",
      "\n",
      "      Acc: 0.6771\n",
      "P/N av. F: 0.5512\n",
      "   Av.rec: 0.5961\n"
     ]
    }
   ],
   "source": [
    "sa_exp('4a.train', '4a.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "featurizing train...\n",
      "0%...done, sparsifying...done!\n",
      "featurizing dev...\n",
      "0%...done, sparsifying...done!\n",
      "training...\n",
      "predicting...\n",
      "evaluating...\n",
      "\n",
      "   label   n_true n_tagged precision   recall  F-score\n",
      " neutral       62       66     0.76     0.81     0.78\n",
      "positive       21       30     0.53     0.76     0.63\n",
      "negative       17        4     0.50     0.12     0.19\n",
      "\n",
      "      Acc: 0.6800\n",
      "P/N av. F: 0.4090\n",
      "   Av.rec: 0.5620\n"
     ]
    }
   ],
   "source": [
    "sa_exp('test.train', 'test.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to get started! Try to improve the main performance figures by implementing new features in the __Featurizer__ class! Make sure that each feature function is a generator and that you add function names to the class variable __feature_functions__. Some ideas for features are listed below, but you should also come up with some ideas on your own:\n",
    "\n",
    "#### Ideas for simple features\n",
    "\n",
    "  - What words are used? Should this be case sensitive?\n",
    "  - What punctuation is used? Should all of them count?\n",
    "  - Do word ngrams help? But for which values of n?\n",
    "  - Emojis?\n",
    "\n",
    "#### Sentiment lexicons\n",
    "There are many on the internet (google is your friend). Just get a couple and use it! \n",
    "\n",
    "#### Some more ideas \n",
    "    \n",
    "   - part-of-speech (POS) tags - try the [POS-tagger in NLTK](http://www.nltk.org/book/ch05.html)\n",
    "   - POS ngrams (but maybe not all of them?)\n",
    "   - can WordNet be of any use? - recall from last week that there's an [NLTK WordNet interface](http://www.nltk.org/howto/wordnet.html)\n",
    "\n",
    "#### Advanced \n",
    "Try to get more info on __rare or unseen words__. You may even want to use the code from last week's excercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
